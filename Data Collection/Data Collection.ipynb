{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures \n",
    "import snscrape.modules.twitter as snscrape\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Collection 1\n",
    "## Tweet Collection\n",
    "\n",
    "Pengumpulan data dimulai dengan mengumpulkan tweets yang berkaitan dengan pemilu. SNScrape digunakan karena kemudahan implementasinya dan ketidakterkaitanya dengan API Twitter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "maxTweets = 1000000\n",
    "count = 0\n",
    "\n",
    "COLUMN_NAME = [\"UserID\",\"User\",\"Location\",\"verified\",\"Date_Created\",\"Follows_Count\",\"Friends_Count\", \n",
    "        \"Retweet_Count\",\"Language\",\"Date_Tweet\",\"Number_of_Likes\",\"Source_of_Tweet\",\n",
    "        \"Tweet_Id\",\"Tweet\",\"Hashtags\",\"Conversation_Id\",\"In_reply_To\",\"Latitude\",\"Longitude\",\"Place\"]\n",
    "df = pd.DataFrame(columns=COLUMN_NAME)\n",
    "\n",
    "keyword = \"#Pilpres2019 OR #DebatPilpres2019 OR #Pemilu2019 OR #BeraniPilih02 OR #JokowiMenangTotalDebat OR #DebatPintarJokowi OR #PrabowoMenangDebat OR #DebatKeduaPilpres2019 OR #Debat02PrabowoMenang OR #DebatCapres OR #PrabowoSandi OR #01JokowiLagi OR #2019PilihJokowi OR #2019GantiPresiden OR #01IndonesiaMaju OR #2019TetapJokowi OR #JokowiLagi OR #2019PrabowoSandi\"\n",
    "date = 'since:2018-09-01 until:2019-05-31 min_faves:1 exclude:retweets'\n",
    "\n",
    "try: \n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' ' + date).get_items()):\n",
    "        if count > maxTweets:\n",
    "            break\n",
    "        df.loc[i] = [\n",
    "                                    tweet.user.id,\n",
    "                                    tweet.user.username,\n",
    "                                    tweet.user.location,\n",
    "                                    tweet.user.verified,\n",
    "                                    tweet.user.created,\n",
    "                                    tweet.user.followersCount,\n",
    "                                    tweet.user.friendsCount,\n",
    "                                    tweet.retweetCount,\n",
    "                                    tweet.lang,\n",
    "                                    tweet.date,\n",
    "                                    tweet.likeCount,\n",
    "                                    tweet.sourceLabel,\n",
    "                                    tweet.id,\n",
    "                                    tweet.content,\n",
    "                                    tweet.hashtags,\n",
    "                                    tweet.conversationId,\n",
    "                                    tweet.inReplyToUser,\n",
    "                                    tweet.coordinates.latitude if tweet.coordinates != None else None,\n",
    "                                    tweet.coordinates.longitude if tweet.coordinates != None else None,\n",
    "                                    tweet.Place.fullName if tweet.place != None else None]\n",
    "        count += 1\n",
    "        print(count)\n",
    "\n",
    "    date = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "    df.to_json(r'Pemilu_' + date + '.json', orient='records', lines=True)\n",
    "    df.to_csv(r'Pemilu_' + date + '.csv', index=False)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"KeyboardInterrupt\")\n",
    "    date = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "    df.to_json(r'Pemilu_Interrupt' + date + '.json', orient='records', lines=True)\n",
    "    df.to_csv(r'Pemilu_Interrupt' + date + '.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode diatas merupakan kode awal yang menggunakan SNScrape dalam mengumpulkan data.  \n",
    "Parameter yang digunakan diantara lain:\n",
    "1. Tanggal pengumpulan data yaitu 1 September 2018 hingga 1 Mei 2019\n",
    "2. Min_faves merupakan like paling sedikit yang diperlukan\n",
    "3. Exclude retweets dan replies dimana retweets dan replies tidak diikutsertakan dalam pengumpulan data  \n",
    "---\n",
    "Keyword yang digunakan merupakan hasil dari riset Twitter yang dipublikasikan dalam [Blog Twitter](https://blog.twitter.com/in_id/topics/events/2019/124-juta-tweet-seputar-pemilihan-umum-2019)  \n",
    "Keyword tersebut merupakan:\n",
    "1. #Pilpres2019\n",
    "2. #DebatPilpres2019\n",
    "3. #Pemilu2019\n",
    "4. #BeraniPilih02\n",
    "5. #JokowiMenangTotalDebat\n",
    "6. #DebatPintarJokowi\n",
    "7. #PrabowoMenangDebat\n",
    "8. #DebatKeduaPilpres2019 \n",
    "9. #Debat02PrabowoMenang \n",
    "10. #DebatCapres \n",
    "11. #PrabowoSandi \n",
    "12. #01JokowiLagi \n",
    "13. #2019PilihJokowi \n",
    "14. #2019GantiPresiden \n",
    "15. #01IndonesiaMaju \n",
    "16. #2019TetapJokowi \n",
    "17. #JokowiLagi \n",
    "18. #2019PrabowoSandi\n",
    "---\n",
    "Hanya 18 keyword tersebut yang dapat dipakai karena keyword dapat digunakan limit library SNScrape.  \n",
    "Pengumpulan data menghasilkan data dengan 418931 tweets.  \n",
    "Hasil Program tersebut merupakan file csv dan json yang dapat digunakan untuk analisis lebih lanjut.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis selanjutnya menentukan lokasi dari tweet tersebut. Lokasi akan ditentukan oleh kolom \"Place\" atau \"Location\" pada dataset tersebut.  \n",
    "Namun, karena kolom tersebut tidak standar dan kadang berisi informasi yang tidak jelas atau tidak terisi sama sekali, maka filtering dilakukan untuk menentukan lokasi mereka."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Lokasi Tweets sebelum cleaning\n",
    "| Lokaksi                        | Count |\n",
    "|--------------------------------|-------|\n",
    "| DKI Jakarta, Indonesia         | 11801 |\n",
    "| Jakarta, Indonesia             | 9792  |\n",
    "| Jakarta                        | 8536  |\n",
    "| Jakarta Capital Region, Indone | 4282  |\n",
    "| Kota Surabaya, Jawa Timur      | 3556  |\n",
    "| Jakarta Capital Region         | 3292  |\n",
    "| Bandung, Jawa Barat            | 3024  |\n",
    "| Yogyakarta, Indonesia          | 2626  |\n",
    "| Jakarta Selatan, DKI Jakarta   | 2279  |\n",
    "| Jawa Barat, Indonesia          | 2133  |\n",
    "| Jakarta Pusat, DKI Jakarta     | 2109  |\n",
    "| indonesia                      | 2054  |\n",
    "| Aceh                           | 1944  |\n",
    "| Sumatera Barat, Indonesia      | 1798  |\n",
    "| Jawa Tengah, Indonesia         | 1788  |\n",
    "| Kota Medan, Sumatera Utara     | 1731  |\n",
    "| INDONESIA                      | 1652  |\n",
    "| Jakarta Timur, DKI Jakarta     | 1538  |\n",
    "| Denpasar Selatan, Indonesia    | 1382  |\n",
    "| mojokerto                      | 1104  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering via Kabutapen Kota\n",
    "Dapat dilihat pada data diatas bahwa lokasi yang diberikan tidak standar. Selain itu, terdapat lokasi yang standard seperti \"indonesia\" dan \"INDONESIA\".  \n",
    "Didasari dengan lokasi yang tidak tentu, filtering dilakukan untuk menentukan lokasi dari user tersebut.  \n",
    "Filtering dilakukan dengan mengecek kolom \"Location\" dengan data kabupaten kota yang ada di Indonesia.  \n",
    "Jika ditemukan, maka lokasi tersebut akan dianggap sebagai lokasi user.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kabupaten_kota merupakan file python yang berisi dictionary dari seluruh provinsi dan kabupaten/kota di Indonesia\n",
    "import kabupaten_kota as kk\n",
    "\n",
    "locations = df['Location'].tolist()\n",
    "\n",
    "def check_kabupaten_kota():\n",
    "    kabupaten_kota_list = []\n",
    "    for loc in locations:\n",
    "        flag = False\n",
    "        # Jika lokasi tidak diketahui, maka akan dianggap sebagai \"Unknown\"\n",
    "        if loc == \"\":\n",
    "            kabupaten_kota_list.append(\"Unknown\")\n",
    "        \n",
    "        # Jika lokasi mengandung nama kabupaten kota, maka akan dianggap sebagai nama kabupaten kota\n",
    "        for prov in kk.provinsi:\n",
    "            for kabkot in kk.kabupaten_kota[prov]:\n",
    "                if kabkot in  str(loc).lower():\n",
    "                    kabupaten_kota_list.append(kabkot)\n",
    "                    flag = True\n",
    "                    break\n",
    "        if flag == False:\n",
    "            kabupaten_kota_list.append(\"Unknown\")\n",
    "        \n",
    "    return kabupaten_kota_list\n",
    "\n",
    "kabupaten_kota_list = check_kabupaten_kota()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Top 20 Kabupaten Kota Tweets setelah filtering\n",
    "| Kabupaten Kota | Count  |\n",
    "|----------------|--------|\n",
    "| Unknown        | 285919 |\n",
    "| jakarta        | 58317  |\n",
    "| bandung        | 8550   |\n",
    "| bekasi         | 4820   |\n",
    "| surabaya       | 4718   |\n",
    "| bogor          | 4430   |\n",
    "| medan          | 4002   |\n",
    "| yogyakarta     | 3747   |\n",
    "| denpasar       | 2487   |\n",
    "| tangerang      | 2394   |\n",
    "| banjar         | 2015   |\n",
    "| semarang       | 1926   |\n",
    "| malang         | 1804   |\n",
    "| depok          | 1723   |\n",
    "| makassar       | 1642   |\n",
    "| sidoarjo       | 1296   |\n",
    "| padang         | 1285   |\n",
    "| bukittinggi    | 1193   |\n",
    "| palembang      | 1187   |\n",
    "| mojokerto      | 1104   |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari total 418931 tweets, 133012 tweets berhasil diidentifikasi lokasinya.  \n",
    "Selain itu, perbedaan jumlah tweets yang diidentifikasi per kabupaten kota juga drastis.  \n",
    "Oleh karena itu, dilakukan filtering lagi untuk menentukan tweets yang didapatkan per provinsi.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering via Provinsi\n",
    "Metode filtering yang dilakukan sama, namun data yang digunakan adalah data provinsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kabupaten_kota as kk \n",
    "\n",
    "locations = df['Location'].tolist()\n",
    "\n",
    "def check_provinsi():\n",
    "    kabupaten_kota_list = []\n",
    "    for loc in locations:\n",
    "        flag = False\n",
    "        # Jika lokasi tidak diketahui, maka akan dianggap sebagai \"Unknown\"\n",
    "        if loc == \"\":\n",
    "            kabupaten_kota_list.append(\"Unknown\")\n",
    "        # Jika lokasi mengandung nama provinsi, maka akan dianggap sebagai nama provinsi\n",
    "        for prov in kk.provinsi:\n",
    "            if prov in str(loc).lower():\n",
    "                kabupaten_kota_list.append(prov)\n",
    "                flag = True\n",
    "                break\n",
    "            # Jika lokasi mengandung nama kabupaten kota, maka akan dianggap sebagai nama provinsi\n",
    "            for kabkot in kk. kabupaten_kota[prov]:\n",
    "                if kabkot in  str(loc).lower():\n",
    "                    kabupaten_kota_list.append(prov)\n",
    "                    flag = True\n",
    "                    break\n",
    "        if flag == False:\n",
    "            kabupaten_kota_list.append(\"Unknown\")\n",
    "\n",
    "\n",
    "provinsi_list = check_provinsi()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Provinsi Tweets setelah filtering\n",
    "| Provinsi            | Count  |\n",
    "|---------------------|--------|\n",
    "| Not Specified       | 112305 |\n",
    "| dki jakarta         | 56910  |\n",
    "| jawa barat          | 25723  |\n",
    "| jawa timur          | 15711  |\n",
    "| jawa tengah         | 7866   |\n",
    "| sumatera barat      | 4875   |\n",
    "| sumatera utara      | 4817   |\n",
    "| aceh                | 4742   |\n",
    "| banten              | 4533   |\n",
    "| bali                | 4261   |\n",
    "| di yogyakarta       | 4151   |\n",
    "| sulawesi selatan    | 2579   |\n",
    "| riau                | 2454   |\n",
    "| nusa tenggara barat | 2212   |\n",
    "| lampung             | 1715   |\n",
    "| kalimantan selatan  | 1710   |\n",
    "| sumatera selatan    | 1430   |\n",
    "| sulawesi utara       | 1087   |\n",
    "| papua               | 1054   |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kesimpulan sama terjadi pada filtering ini dimana jumlah tweets yang diidentifikasi per provinsi juga berbeda drastis.    \n",
    "Ide lain dengan menggunakan commenter atau follower dari akun politik pada kabupaten / kota dilontarkan untuk mencoba mendapatkan data yang lebih banyak.  \n",
    "Tetapi, menemukan akun politik yang memiliki follower atau commenter yang banyak juga tidak mudah.  \n",
    "Disini juga diputuskan bahwa scope dari penelitian hanya sampai di pulau Jawa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Collection 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk memperbesar dataset, scraping kembali dilakukan dengan menggunakan keyword yang sama. Untuk mempercepat proses pengumpulan tweets, algoritma yang digunakan dimodifikasi agar menggunakan multithreading.  \n",
    "\n",
    "Digunakan module concurrent.futures untuk melakukan multithreading. Setelah dilakukan pengumpulan data, didapatkan 3965628 tweets dan 2847038 tweets yang memiliki lokasi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena hasil yang masih imbalance, lokasi user unique yang memiliki lokasi digunakan untuk membuat dataset baru yang berisi dengan tweets yang memiliki di pulau jawa tersebut.  \n",
    "Dengan menggunakan fungsi check_provinsi diatas, hasil yang didapatkan adalah 13616 user yang memiliki lokasi di pulau jawa.  \n",
    "## Contoh hasil check_provinsi\n",
    "\n",
    "| Username         | Provinsi    |\n",
    "|------------------|-------------|\n",
    "| warsahidayat     | jawa barat  |\n",
    "| Hasanudin        | jawa barat  |\n",
    "| serujinews       | jawa timur  |\n",
    "| detikcom         | dki jakarta |\n",
    "| okezonenews      | dki jakarta |\n",
    "| banu_sukamto     | dki jakarta |\n",
    "| netgrit          | dki jakarta |\n",
    "| inewsdotid       | dki jakarta |\n",
    "| dialogikapod     | dki jakarta |\n",
    "| fadjroeL         | dki jakarta |\n",
    "| hariankompas     | dki jakarta |\n",
    "| mahadewa         | dki jakarta |\n",
    "| HansiPenjaga     | jawa barat  |\n",
    "| bambang_samekto  | jawa tengah |\n",
    "| Tarikh_Tambang   | jawa timur  |\n",
    "| MataNajwa        | dki jakarta |\n",
    "| hakasasiid       | dki jakarta |\n",
    "| kunartomarzuki   | jawa tengah |\n",
    "| SeputariNews     | dki jakarta |\n",
    "| mad_pecci        | jawa barat  |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering tweets\n",
    "Karena tweets yang dibuat oleh user tidak selalu berkaitan dengan politik, maka dilakukan filtering dengan menggunakan keyword yang sama dengan scraping.  \n",
    "Keyword dibuat dengan menggunakan sample 100000 tweets yang diambil dari dataset yang sudah dikumpulkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "import re,emoji\n",
    "\n",
    "\n",
    "\n",
    "def find_url(string):\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    # regex untuk mencari seluruh url pada string\n",
    "    url = re.findall(regex,string)\n",
    "    # mengembalikan list url\n",
    "    return [x[0] for x in url]\n",
    "\n",
    "def get_word_count(sample):\n",
    "\n",
    "    lexicon = {}\n",
    "    for i in sample:\n",
    "        # menghapus emoji\n",
    "        i = emoji.replace_emoji(i,'').lower()\n",
    "        for word in i.split():\n",
    "            # Mengganti username menjadi @USER\n",
    "            if word[0] == '@' or word == '[username]':\n",
    "                word = '@USER'\n",
    "            # Mengganti url menjadi HTTPURL\n",
    "            elif find_url(word) != []:\n",
    "                word = 'HTTPURL'\n",
    "            elif word == 'httpurl' or word == '[url]':\n",
    "                word = 'HTTPURL'\n",
    "            \n",
    "            # Menghapus tanda baca\n",
    "            word = re.sub(r'[^\\w\\s]','',word)\n",
    "\n",
    "            # menambahkan kata ke lexicon\n",
    "            if (word != 'HTTPURL' or word != 'USER') and word not in lexicon:\n",
    "                lexicon[word] = 1\n",
    "            else:\n",
    "                lexicon[word] += 1\n",
    "    return lexicon \n",
    "\n",
    "\n",
    "df = pd.read_csv('Data/2022-11-12_00_21_00dropped.csv')\n",
    "tweets = df['content'].tolist()\n",
    "sampling = random.sample(tweets, 100000)\n",
    "lexicon = get_word_count(sampling)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu data akan dicek manual untuk menentukan apakah kata tersebut berkaitan dengan politik atau tidak.  \n",
    "Contoh data yang dihasilkan dari filtering adalah sebagai berikut:\n",
    "\n",
    "| text                | count  | keyword |\n",
    "|---------------------|--------|---------|\n",
    "| prabowobersamaulama | 181    | 1       |\n",
    "| bangsa              | 1043   | 1       |\n",
    "| nasional            | 204    | 1       |\n",
    "| dpr                 | 280    | 1       |\n",
    "| 1                   | 1114   | 1       |\n",
    "| USER                | 372376 | 0       |\n",
    "| yg                  | 23425  | 0       |\n",
    "| HTTPURL             | 21844  | 0       |\n",
    "| NaN                 | 18236  | 0       |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection 3 dengan scope Pulau Jawa dan filtering menggunakan keyword\n",
    "\n",
    "Scraping kembali dilakukan untuk memperbanyak tweets yang didapatkan. Pengumpulan tweets dilakukan kedua hasil filtering diatas.  \n",
    "Data tersebut didapatkan dari user unique yang didapatkan dari data sebelumnya.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import concurrent.futures \n",
    "import snscrape.modules.twitter as snscrape\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "words = pd.read_csv('keyword.csv', delimiter=';', encoding = \"ISO-8859-1\")\n",
    "words = words[words['keyword'] == 1]\n",
    "\n",
    "filtered_words = words['text'].tolist()\n",
    "\n",
    "def filter_data(text):\n",
    "    # Jika tweet mengandung kata kunci, maka akan dianggap sebagai data yang relevan\n",
    "    if any(word in text for word in filtered_words):\n",
    "        return True\n",
    "\n",
    "\n",
    "def get_tweets(username):\n",
    "    results = []\n",
    "    username = username.strip()\n",
    "\n",
    "    # parameter untuk pencarian tweet\n",
    "    keyword = \"#from:\" + username\n",
    "    date = 'since:2018-09-01 until:2019-05-31 min_faves:1 exclude:retweets'\n",
    "\n",
    "    for i,tweet in enumerate(snscrape.TwitterSearchScraper(keyword + \" \" + date).get_items()):\n",
    "        # Jika tweet mengandung kata kunci, maka akan dianggap sebagai data yang relevan\n",
    "        if (filter_data(tweet.content)):\n",
    "            results.append([username, tweet.user.location, tweet.id, tweet.content, tweet.renderedContent, tweet.inReplyToTweetId, tweet.inReplyToUser, tweet.likeCount, tweet.retweetCount, tweet.quoteCount, tweet.replyCount, tweet.date])\n",
    "        # Jika sudah mendapatkan 2000 tweet, maka akan berhenti\n",
    "        if (i == 2000):\n",
    "            break\n",
    "    print(\"Done getting tweets from \", username)\n",
    "    return results\n",
    "\n",
    "\n",
    "def concurrent_get_tweets(usernames):\n",
    "    count = 0\n",
    "    result = []\n",
    "        \n",
    "    # Inisialisasi thread pool dengan 6 thread\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        # Memulai thread untuk setiap username\n",
    "        futures = [executor.submit(get_tweets, username) for username in usernames]\n",
    "        # Inisialisasi dataframe untuk autosave\n",
    "        autosave = pd.DataFrame(columns=['username', 'location','tweetID', 'content', 'renderedContent', 'inReplyToTweetId', 'inReplyToUser', 'likeCount', 'retweetCount', 'quoteCount', 'replyCount', 'date'])\n",
    "        # Menunggu semua thread selesai\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                # Mengambil hasil dari thread\n",
    "                data = future.result()\n",
    "                result.extend(data)\n",
    "                count += 1\n",
    "                # Autosave setiap 100 user\n",
    "                if (count % 100 == 0):   \n",
    "                    print(\"Done getting \", count, \" out of \", len(usernames) , \" users\")\n",
    "                    autosave = pd.DataFrame(result, columns=['username', 'location','tweetID', 'content', 'renderedContent', 'inReplyToTweetId', 'inReplyToUser', 'likeCount', 'retweetCount', 'quoteCount', 'replyCount', 'date'])\n",
    "                    autosave.to_csv('autosave_jawa.csv', index=False)\n",
    "                    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    print(\"Autosaved at \", now)\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "                pass\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didapatkan tweets sebanyak 1943849 tweets dari 13616 user yang memiliki lokasi di pulau jawa.  \n",
    "Setiap user memiliki dapat menambahkan maksimal 2000 tweets pada dataset.\n",
    "## Pembagian provinsi hasil scraping\n",
    "\n",
    "| Provinsi    | Count   |\n",
    "|-------------|---------|\n",
    "| dki jakarta | 1067556 |\n",
    "| jawa barat  | 422217  |\n",
    "| jawa timur  | 291467  |\n",
    "| jawa tengah | 162609  |\n",
    "\n",
    "Berdasarkan hasil tersebut, undersampling digunakan untuk menyeimbangkan data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "Hasil akhir sampling adalah 600000 tweets dengan 150000 tweets per provinsi.\n",
    "| Provinsi    | Count  |\n",
    "|-------------|--------|\n",
    "| dki jakarta | 150000 |\n",
    "| jawa barat  | 150000 |\n",
    "| jawa tengah | 150000 |\n",
    "| jawa timur  | 150000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMax = 150000\n",
    "\n",
    "sampled = df.groupby('provinsi').apply(lambda x: x.sample(n=min(nMax, len(x))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
