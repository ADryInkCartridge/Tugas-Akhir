{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "df = pd.read_csv('Data/Dataset.csv', decimal =\".\", thousands=\",\")\n",
    "df = df.drop(['Kota','Paslon 1','Paslon 2','Total'],axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_minmax = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "dfX, dfY = df_minmax.iloc[:, :-1], df_minmax.iloc[:, [-1]]\n",
    "kf = RepeatedKFold(n_splits=10, n_repeats = 3 , random_state=1)\n",
    "kf.get_n_splits(dfX)\n",
    "y = df['Partisipasi']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "3 metrik akan digunakan untuk mengevaluasi model yang telah dibuat, yaitu:\n",
    "1. Mean Absolute Error (MAE)\n",
    "- MAE merupakan rata - rata dari selisih absolut antara nilai prediksi dengan nilai aktual. MAE merupakan metrik yang paling mudah diinterpretasikan karena tidak melibatkan satuan. MAE yang semakin kecil menunjukkan model yang semakin baik.\n",
    "2. Root Mean Squared Error (RMSE)\n",
    "- RMSE merupakan akar kuadrat dari rata - rata dari selisih kuadrat antara nilai prediksi dengan nilai aktual. RMSE yang semakin kecil menunjukkan model yang semakin baik.\n",
    "3. R-Squared (R2)\n",
    "- R2 merupakan koefisien determinasi yang menunjukkan seberapa baik model yang dibuat dapat menjelaskan variabilitas dari data yang diamati. R2 yang semakin besar menunjukkan model yang semakin baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def error_rate(methods,preds,vals):\n",
    "  rmse = math.sqrt(mean_squared_error(vals,preds))\n",
    "  mae = mean_absolute_error(vals, preds)\n",
    "  r2 = r2_score(vals, preds)\n",
    "  \n",
    "  return methods,rmse,mae,r2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Sebelum seleksi fitur, akan dilakukan pemodelan dengan menggunakan semua fitur yang ada. Pemodelan dilakukan terlebih dahulu untuk mencari model terbaik yang akan digunakan pada seleksi fitur.  \n",
    "Pemodelan dilakukan dengan menggunakan 7 algoritma, yaitu:\n",
    "1. Linear Regression\n",
    "2. SVR\n",
    "3. Decision Tree\n",
    "4. Random Forest\n",
    "5. XGBoost\n",
    "6. LightGBM\n",
    "7. CatBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten\n",
    "\n",
    "Flatten merupakan sebuah fungsi utilitas untuk mengubah array 2 dimensi menjadi array 1 dimensi. Fungsi ini akan digunakan untuk mengubah array 2 dimensi hasil prediksi menjadi array 1 dimensi agar dapat digunakan pada metrik evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    # flatten list of list menjadi sebuah list\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Linear Regression',\n",
       " 0.041151108859387615,\n",
       " 0.03339395849373654,\n",
       " 0.2803149971364153)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_linreg(dfX,y,kf):\n",
    "  reg = LinearRegression()\n",
    "  vals = []\n",
    "  preds = []\n",
    "  for train_index, test_index in kf.split(dfX):\n",
    "      X_train, X_test = dfX.iloc[train_index], dfX.iloc[test_index]\n",
    "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      reg.fit(X_train, y_train)\n",
    "      pred = reg.predict(X_test)\n",
    "\n",
    "      preds.append(pred)\n",
    "      vals.append(y_test)\n",
    "  linear_reg_df = pd.DataFrame({'Prediction':flatten(preds),'Actual Values':flatten(vals)})\n",
    "  return error_rate(\"Linear Regression\",linear_reg_df['Prediction'],linear_reg_df['Actual Values'])\n",
    "\n",
    "evaluate_linreg(dfX, y, kf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SVR', 0.04773394043795352, 0.03884803361188745, 0.03164646558058193)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "def evaluate_svr(dfX,y,kf):\n",
    "  \n",
    "  svr_reg = SVR()\n",
    "  vals = []\n",
    "  preds = []\n",
    "\n",
    "  for train_index, test_index in kf.split(dfX):\n",
    "      X_train, X_test = dfX.iloc[train_index], dfX.iloc[test_index]\n",
    "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      svr_reg.fit(X_train,y_train)\n",
    "      svr_pred = svr_reg.predict(X_test)\n",
    "      \n",
    "      preds.append(svr_pred)\n",
    "      vals.append(y_test)\n",
    "\n",
    "  svr_df = pd.DataFrame({'Prediction':flatten(preds),'Actual Values':flatten(vals)})\n",
    "  return error_rate(\"SVR\",svr_df['Prediction'],svr_df['Actual Values'])\n",
    "\n",
    "evaluate_svr(dfX, y, kf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Decision Tree',\n",
       " 0.05523462757282909,\n",
       " 0.04503630844654088,\n",
       " -0.2965886883579478)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def evaluate_dt(dfX,y,kf):\n",
    "  \n",
    "  dt_reg = DecisionTreeRegressor()\n",
    "  vals = []\n",
    "  preds = []\n",
    "  for train_index, test_index in kf.split(dfX):\n",
    "      \n",
    "      X_train, X_test = dfX.iloc[train_index], dfX.iloc[test_index]\n",
    "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      dt_reg.fit(X_train,y_train)\n",
    "      dt_pred = dt_reg.predict(X_test)\n",
    "\n",
    "      preds.append(dt_pred)\n",
    "      vals.append(y_test)\n",
    "\n",
    "  dt_df = pd.DataFrame({'Prediction':flatten(preds),'Actual Values':flatten(vals)})\n",
    "  return error_rate(\"Decision Tree\",dt_df['Prediction'],dt_df['Actual Values'])\n",
    "  \n",
    "evaluate_dt(dfX,y,kf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Random Forest',\n",
       " 0.04015124271912773,\n",
       " 0.03232011222889938,\n",
       " 0.31486311058980954)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def evaluate_rf(dfX,y,kf):\n",
    "  \n",
    "  vals = []\n",
    "  preds = []\n",
    "\n",
    "  rf = RandomForestRegressor(n_estimators= 100, random_state = 0)\n",
    "\n",
    "  for train_index, test_index in kf.split(dfX):\n",
    "      X_train, X_test = dfX.iloc[train_index], dfX.iloc[test_index]\n",
    "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      rf.fit(X_train,y_train)\n",
    "      rf_pred = rf.predict(X_test)\n",
    "\n",
    "      preds.append(rf_pred)\n",
    "      vals.append(y_test)\n",
    "\n",
    "  rf_df = pd.DataFrame({'Prediction':flatten(preds),'Actual Values':flatten(vals)})\n",
    "  return error_rate(\"Random Forest\",rf_df['Prediction'],rf_df['Actual Values'])\n",
    "\n",
    "evaluate_rf(dfX,y,kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('XGBoost', 0.04671284884163465, 0.03846689642337487, 0.07263206290551683)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "def evaluate_xgboost(dfX,y,kf):\n",
    "  \n",
    "  vals = []\n",
    "  preds = []\n",
    "\n",
    "  xgb = XGBRegressor()\n",
    "\n",
    "  for train_index, test_index in kf.split(dfX):\n",
    "      X_train, X_test = dfX.iloc[train_index], dfX.iloc[test_index]\n",
    "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      xgb.fit(X_train,y_train)\n",
    "      xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "      preds.append(xgb_pred)\n",
    "      vals.append(y_test)\n",
    "\n",
    "  xgb_df = pd.DataFrame({'Prediction':flatten(preds),'Actual Values':flatten(vals)})\n",
    "  return error_rate(\"XGBoost\",xgb_df['Prediction'],xgb_df['Actual Values'])\n",
    "\n",
    "evaluate_xgboost(dfX,y,kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LightGBM', 0.04281453667792947, 0.034679282965554566, 0.22095621803714005)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb  \n",
    "\n",
    "def evaluate_lightgbm(dfX,y):\n",
    "  \n",
    "  params = {\n",
    "      'task': 'train', \n",
    "      'boosting': 'gbdt',\n",
    "      'objective': 'regression',\n",
    "      'num_leaves': 10,\n",
    "      'learning_rate': 0.05,\n",
    "      'metric': {'l2','l1'},\n",
    "      'verbose': -1\n",
    "  }\n",
    "  \n",
    "  vals = []\n",
    "  preds = []\n",
    "\n",
    "  for train_index, test_index in kf.split(dfX):\n",
    "\n",
    "      X_train, X_test = dfX.iloc[train_index], dfX.iloc[test_index]\n",
    "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "      lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "      lgb_reg = lgb.train(params,train_set=lgb_train)\n",
    "      lgb_pred = lgb_reg.predict(X_test)\n",
    "      \n",
    "      preds.append(lgb_pred)\n",
    "      vals.append(y_test)\n",
    "\n",
    "  lgb_df = pd.DataFrame({'Prediction':flatten(preds),'Actual Values':flatten(vals)})\n",
    "  return error_rate(\"LightGBM\",lgb_df['Prediction'],lgb_df['Actual Values'])\n",
    "evaluate_lightgbm(dfX,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Catboost', 0.04001594428986717, 0.0326771489029961, 0.3194727692532964)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool\n",
    "\n",
    "def evaluate_catboost(dfX,y):\n",
    "  vals = []\n",
    "  preds = []\n",
    "\n",
    "  cbr = CatBoostRegressor()\n",
    "  for train_index, test_index in kf.split(dfX):\n",
    "      X_train, X_test = dfX.iloc[train_index], dfX.iloc[test_index]\n",
    "      y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "      \n",
    "      train_pool = Pool(data=X_train, label=y_train)\n",
    "      test_pool = Pool(data=X_test, label=y_test)\n",
    "\n",
    "      cbr.fit(train_pool, verbose=False)\n",
    "      cbr_pred = cbr.predict(test_pool)\n",
    "      \n",
    "      preds.append(cbr_pred)\n",
    "      vals.append(y_test)\n",
    "\n",
    "  cat_df = pd.DataFrame({'Prediction':flatten(preds),'Actual Values':flatten(vals)})\n",
    "  return error_rate(\"Catboost\",cat_df['Prediction'],cat_df['Actual Values'])\n",
    "\n",
    "evaluate_catboost(dfX,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "\n",
    "Pada modelling pertama, terlihat metode - metode yang menggunakan ensemble learning seperti random forest dan gradient boosting seperti XGBoost, LightGBM, dan CatBoost memiliki performa yang lebih baik dibandingkan dengan metode - metode lainnya.  Selain itu, linear regression juga mempunyai performa yang cukup baik dengan waktu training yang sangat cepat.  \n",
    "Oleh karena itu, feature selection akan dilakukan dengan menggunakan metode - metode tersebut.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
