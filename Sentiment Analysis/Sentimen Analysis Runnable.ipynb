{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nlp_id.tokenizer import Tokenizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from nlp_id.stopword import StopWord \n",
    "from nltk.corpus import stopwords\n",
    "import words as w\n",
    "\n",
    "class normalizer():\n",
    "    def __init__(self):\n",
    "        nltk.download('stopwords')\n",
    "        stopwords_sastrawi = StopWordRemoverFactory()\n",
    "        stopwords_nlpid = StopWord() \n",
    "        stopwords_nltk = stopwords.words('indonesian')\n",
    "        stopwords_github = list(np.array(pd.read_csv(\"Utils/stopwords.txt\", header=None).values).squeeze())\n",
    "        more_stopword = w.custom_stopwords\n",
    "        data_stopword = stopwords_sastrawi.get_stop_words() + stopwords_nlpid.get_stopword() + stopwords_github + stopwords_nltk + more_stopword \n",
    "        data_stopword = list(set(data_stopword))\n",
    "\n",
    "        # Only use 'rt' as stopwords\n",
    "        data_stopword = list(set(data_stopword))\n",
    "\n",
    "        # Combine slang dictionary\n",
    "        import json\n",
    "        with open('Utils/slang.txt') as f:\n",
    "            data = f.read()\n",
    "        data_slang = json.loads(data) \n",
    "\n",
    "        with open('Utils/sinonim.txt') as f:\n",
    "            data = f.readlines()\n",
    "        for line in data:\n",
    "            word = line.split('=')\n",
    "            data_slang[word[0].strip()] = word[1].strip()\n",
    "\n",
    "        # print(data_slang)\n",
    "        more_dict = w.custom_dict\n",
    "        data_slang.update(more_dict)\n",
    "\n",
    "        self.stopwords, self.slang = data_stopword, data_slang\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "    def normalize(self,text):\n",
    "        text = text.lower()\n",
    "  \n",
    "        # Change HTML entities\n",
    "        text = text.replace('&amp;', 'dan')\n",
    "        text = text.replace('&gt;', 'lebih dari')\n",
    "        text = text.replace('&lt;', 'kurang dari')\n",
    "        \n",
    "        # Remove url\n",
    "        text = re.sub(r'http\\S+', 'httpurl', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        \n",
    "        # Remove hashtags\n",
    "        text = re.sub(r'#\\w+', ' ', text)\n",
    "        \n",
    "        # Replace @mentions with 'user'\n",
    "        text = re.sub(r'@\\w+', 'user', text)\n",
    "\n",
    "        # Remove non-letter characters\n",
    "        text = re.sub('[^a-zA-z]', ' ', text)\n",
    "\n",
    "        # Remove excess space\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.strip()\n",
    "\n",
    "        result = []\n",
    "        word_token = self.tokenizer.tokenize(text) # Tokenize words\n",
    "        for word in word_token:\n",
    "            word = word.strip().lower() # Case Folding to Lower Case\n",
    "            if word in self.slang:\n",
    "                word = self.slang[word]\n",
    "            if word not in self.stopwords: # Stopwords removal\n",
    "                result.append(word)\n",
    "            else:\n",
    "                continue\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Bayu Adjie\n",
      "[nltk_data]     Sidharta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coba', 'bayangkan', 'ketua', 'mk', 'ketua', 'panglima', 'tni', 'jaksa', 'agung', 'ketua', 'kpk', 'kepala', 'bin', 'kapolri', 'alasan', 'cawe', 'memenangkan', 'capres', 'cawapres', 'pemilu', 'maksud']\n"
     ]
    }
   ],
   "source": [
    "normalize = normalizer()\n",
    "test_text = \"Luar biasa! Coba kita bayangkan apa yg bakal terjadi jika Ketua MK, Ketua MA, Panglima TNI, Jaksa Agung, Ketua KPK, Kepala BIN, dan Kapolri juga dgn menggunakan alasan yg sama ikut cawe2 dlm memenangkan Capres-Cawapres tertentu dlm Pemilu 2024? Itukah maksudnya?#RakyatMonitor#\"\n",
    "print(normalize.normalize(test_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.corpus.reader import WordNetError\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nlp_id.tokenizer import Tokenizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from nlp_id.stopword import StopWord \n",
    "from nltk.corpus import stopwords\n",
    "import words as w\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "\n",
    "class SentiSynset:\n",
    "    def __init__(self, pos_score, neg_score, synset):\n",
    "        self._pos_score = pos_score\n",
    "        self._neg_score = neg_score\n",
    "        self._obj_score = 1.0 - (self._pos_score + self._neg_score)\n",
    "        self.synset = synset\n",
    "\n",
    "\n",
    "    def pos_score(self):\n",
    "        return self._pos_score\n",
    "\n",
    "\n",
    "    def neg_score(self):\n",
    "        return self._neg_score\n",
    "\n",
    "\n",
    "    def obj_score(self):\n",
    "        return self._obj_score\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Prints just the Pos/Neg scores for now.\"\"\"\n",
    "        s = \"<\"\n",
    "        s += self.synset.name() + \": \"\n",
    "        s += \"PosScore=%s \" % self._pos_score\n",
    "        s += \"NegScore=%s\" % self._neg_score\n",
    "        s += \">\"\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Senti\" + repr(self.synset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomSentiWordNet(object):\n",
    "    def __init__(self):\n",
    "        with open(\"Utils/barasa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        # create empty 2d dict\n",
    "        synsets = {}\n",
    "        id_dict = {}\n",
    "        for line in lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 6:\n",
    "                continue\n",
    "            synset_id = parts[0]\n",
    "\n",
    "            if synset_id not in synsets:\n",
    "                synsets[synset_id] = {}\n",
    "            \n",
    "            synset = {}\n",
    "            id, lang, goodness, lemma, pos, neg = parts\n",
    "            pos = float(pos)\n",
    "            neg = float(neg)\n",
    "            synsets[synset_id][lemma] = (pos, neg, 1 - (pos + neg))\n",
    "            id_dict[lemma] = synset_id\n",
    "\n",
    "        self.lemma_dict = id_dict\n",
    "        self.synsets = synsets\n",
    "        self.not_found = {}\n",
    "    \n",
    "    def _get_synset(self, synset_id):\n",
    "        # helper function to map synset_id to synset\n",
    "        synsets = self.synsets[synset_id]\n",
    "        return synsets\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _get_pos_file(self, pos):\n",
    "        # helper function to map WordNet POS tags to file names\n",
    "        if pos == 'n':\n",
    "            return 'noun'\n",
    "        elif pos == 'v':\n",
    "            return 'verb'\n",
    "        elif pos == 'a' or pos == 's':\n",
    "            return 'adj'\n",
    "        elif pos == 'r':\n",
    "            return 'adv'\n",
    "        else:\n",
    "            raise WordNetError('Unknown POS tag: {}'.format(pos))\n",
    "    \n",
    "    \n",
    "    def senti_synset(self, synset_id):\n",
    "        pos_score,neg_score,obj_score = self.synsets[synset_id]\n",
    "        synset = self._get_synset(synset_id)\n",
    "        return SentiSynset(synset, pos_score, neg_score)\n",
    "    \n",
    "    def calculate_sentiment(self,tokens):\n",
    "        pos = []\n",
    "        neg = []\n",
    "        found = []\n",
    "        for token in tokens:\n",
    "            if token not in self.lemma_dict:\n",
    "                self.not_found[token] = self.not_found.get(token, 0) + 1\n",
    "                continue\n",
    "            synsets = self.synsets[self.lemma_dict[token]][token]\n",
    "            pos_score, neg_score, obj_score = synsets\n",
    "            # print(token)\n",
    "            print(\"Found {} with pos {} and neg {}\".format(token, pos_score, neg_score))\n",
    "            pos.append(pos_score)\n",
    "            neg.append(neg_score)\n",
    "            found.append(token)\n",
    "        print(\"Found {} out of {} tokens\".format(len(found), len(tokens)))\n",
    "        print(\", \".join(found))\n",
    "        print(\"Unique tokens found: {}\".format(len(set(found))))\n",
    "        return pos, neg\n",
    "    \n",
    "    def get_not_found(self):\n",
    "        return self.not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found coba with pos 0.0 and neg 0.0\n",
      "Found bayangkan with pos 0.0 and neg 0.0\n",
      "Found ketua with pos 0.0 and neg 0.0\n",
      "Found ketua with pos 0.0 and neg 0.0\n",
      "Found panglima with pos 0.0 and neg 0.0\n",
      "Found jaksa with pos 0.0 and neg 0.0\n",
      "Found agung with pos 0.0 and neg 0.0\n",
      "Found ketua with pos 0.0 and neg 0.0\n",
      "Found kepala with pos 0.0 and neg 0.0\n",
      "Found alasan with pos 0.625 and neg 0.0\n",
      "Found memenangkan with pos 0.125 and neg 0.0\n",
      "Found maksud with pos 0.0 and neg 0.125\n",
      "Found 12 out of 21 tokens\n",
      "coba, bayangkan, ketua, ketua, panglima, jaksa, agung, ketua, kepala, alasan, memenangkan, maksud\n",
      "Unique tokens found: 10\n",
      "([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.125, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125])\n"
     ]
    }
   ],
   "source": [
    "barasa = CustomSentiWordNet()\n",
    "print(barasa.calculate_sentiment(normalize.normalize(test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mk': 1,\n",
       " 'tni': 1,\n",
       " 'kpk': 1,\n",
       " 'bin': 1,\n",
       " 'kapolri': 1,\n",
       " 'cawe': 1,\n",
       " 'capres': 1,\n",
       " 'cawapres': 1,\n",
       " 'pemilu': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barasa.get_not_found()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "def read_inset(path):\n",
    "    sentiments = {}\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        word, sentiment = line.split('\\t')\n",
    "        sentiments[word] = int(sentiment)\n",
    "    print(len(sentiments))\n",
    "    return sentiments\n",
    "\n",
    "def print_n_grams(unigrams, bigrams, trigrams):\n",
    "    print('Unigrams: ', ', '.join(unigrams))\n",
    "    print('Bigrams: ', ', '.join(bigrams))\n",
    "    print('Trigrams: ', ', '.join(trigrams))\n",
    "\n",
    "    \n",
    "\n",
    "class inSet():\n",
    "    def __init__(self, verbose = False):\n",
    "        self.pos = read_inset('Utils/Inset/positive.tsv')\n",
    "        self.neg = read_inset('Utils/Inset/negative.tsv')\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def delete_word_from_text(self, text, word):\n",
    "        text = text.replace(word, '', 1)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def calculate_n_gram(self, text):\n",
    "        unigrams = ngrams(text.split(), 1)\n",
    "        bigrams = ngrams(text.split(), 2)\n",
    "        trigrams = ngrams(text.split(), 3)\n",
    "\n",
    "        unigrams = [' '.join(grams) for grams in unigrams]\n",
    "        bigrams = [' '.join(grams) for grams in bigrams]\n",
    "        trigrams = [' '.join(grams) for grams in trigrams]\n",
    "\n",
    "        return unigrams, bigrams, trigrams\n",
    "    \n",
    "    def recalculate_n_grams(self, text, word):\n",
    "        text = self.delete_word_from_text(text, word)\n",
    "        unigrams, bigrams, trigrams = self.calculate_n_gram(text)\n",
    "        if self.verbose:\n",
    "            print_n_grams(unigrams, bigrams, trigrams)\n",
    "        return unigrams, bigrams, trigrams, text\n",
    "\n",
    "    def calculate_inset_score(self, text):\n",
    "        text_length = len(text.split())\n",
    "        unigrams, bigrams, trigrams = self.calculate_n_gram(text)\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        found = []\n",
    "        for trigram in trigrams:\n",
    "            if trigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Trigram Pos ', trigram)\n",
    "                print('Positive Trigram {} with score {}'.format(trigram, self.pos[trigram]))\n",
    "                pos_score += self.pos[trigram]\n",
    "                found.append(trigram)\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, trigram)\n",
    "            if trigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Trigram Neg ', trigram)\n",
    "                print('Negative Trigram {} with score {}'.format(trigram, self.neg[trigram]))\n",
    "                neg_score += self.neg[trigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, trigram)\n",
    "        \n",
    "\n",
    "        for bigram in bigrams:\n",
    "            if bigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Bigram Pos ', bigram)\n",
    "                print('Positive Bigram {} with score {}'.format(bigram, self.pos[bigram]))\n",
    "                pos_score += self.pos[bigram]\n",
    "                found.append(bigram)\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, bigram)\n",
    "\n",
    "            if bigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Bigram Neg ', bigram)\n",
    "                print('Negative Bigram {} with score {}'.format(bigram, self.neg[bigram]))\n",
    "                neg_score += self.neg[bigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, bigram)\n",
    "\n",
    "        for unigram in unigrams:\n",
    "            if unigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Unigram Pos ', unigram)\n",
    "                print('Positive Unigram {} with score {}'.format(unigram, self.pos[unigram]))\n",
    "                pos_score += self.pos[unigram]\n",
    "                found.append(unigram)\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, unigram)\n",
    "\n",
    "            if unigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Unigram Neg ', unigram)\n",
    "                print('Negative Unigram {} with score {}'.format(unigram, self.neg[unigram]))   \n",
    "                neg_score += self.neg[unigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, unigram)\n",
    "        print(\"Found {} out of {} tokens\".format(len(found), text_length))\n",
    "        print(\", \".join(found))\n",
    "        print(\"Unique tokens found: {}\".format(len(set(found))))\n",
    "        return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3609\n",
      "6607\n",
      "Positive Unigram coba with score 2\n",
      "Negative Unigram coba with score -1\n",
      "Positive Unigram panglima with score 3\n",
      "Positive Unigram maksud with score 3\n",
      "Negative Unigram maksud with score -1\n",
      "Found 3 out of 21 tokens\n",
      "coba, panglima, maksud\n",
      "Unique tokens found: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, -2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insett = inSet(verbose=False)\n",
    "insett.calculate_inset_score(' '.join(normalize.normalize(test_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndoBERTweet Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--bert_model {indobert,indobertweet}]\n",
      "                             [--data_path DATA_PATH] [--output_dir OUTPUT_DIR]\n",
      "                             [--max_token MAX_TOKEN] [--batch_size BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--logging_steps LOGGING_STEPS] [--seed SEED]\n",
      "                             [--local_rank LOCAL_RANK] [--patience PATIENCE]\n",
      "                             [--no_cuda NO_CUDA]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"281d14dc-e5f0-4f9f-8bd0-fca4b0d97102\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=\"c:\\Users\\Bayu Adjie Sidharta\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-14676ZQvSurp65D0X.json\"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bayu Adjie Sidharta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json, glob, os, random\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import re, emoji\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "model_dict = { 'indobertweet': 'indolem/indobertweet-base-uncased',\n",
    "               'indobert': 'indolem/indobert-base-uncased'}\n",
    "\n",
    "\n",
    "def find_url(string):\n",
    "    # with valid conditions for urls in string \n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)\n",
    "    return [x[0] for x in url]\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = emoji.demojize(tweet).lower()\n",
    "    new_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word[0] == '@' or word == '[username]':\n",
    "            new_tweet.append('@USER')\n",
    "        elif find_url(word) != []:\n",
    "            new_tweet.append('HTTPURL')\n",
    "        elif word == 'httpurl' or word == '[url]':\n",
    "            new_tweet.append('HTTPURL')\n",
    "        else:\n",
    "            new_tweet.append(word)\n",
    "    return ' '.join(new_tweet)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "class BertData():\n",
    "    def __init__(self, args):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dict[args.bert_model], do_lower_case=True)\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "        self.MAX_TOKEN = args.max_token\n",
    "\n",
    "    def preprocess_one(self, src_txt):\n",
    "        src_txt = preprocess_tweet(src_txt)\n",
    "        src_subtokens = [self.cls_token] + self.tokenizer.tokenize(src_txt) + [self.sep_token]        \n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        \n",
    "        if len(src_subtoken_idxs) > self.MAX_TOKEN:\n",
    "            src_subtoken_idxs = src_subtoken_idxs[:self.MAX_TOKEN]\n",
    "            src_subtoken_idxs[-1] = self.sep_vid\n",
    "        else:\n",
    "            src_subtoken_idxs += [self.pad_vid] * (self.MAX_TOKEN-len(src_subtoken_idxs))\n",
    "        segments_ids = [0] * len(src_subtoken_idxs)\n",
    "        assert len(src_subtoken_idxs) == len(segments_ids)\n",
    "        return src_subtoken_idxs, segments_ids\n",
    "    \n",
    "    def preprocess(self, src_txts):\n",
    "        output = []\n",
    "        for idx in range(len(src_txts)):\n",
    "            output.append(self.preprocess_one(src_txts[idx]))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, data, idx, batch_size, device):\n",
    "        cur_batch = data[idx:idx+batch_size]\n",
    "        src = torch.tensor([x[0] for x in cur_batch])\n",
    "        seg = torch.tensor([x[1] for x in cur_batch])\n",
    "        # label = torch.tensor([x[2] for x in cur_batch])\n",
    "        mask_src = 0 + (src != 0)\n",
    "        \n",
    "        self.src = src.to(device)\n",
    "        self.seg= seg.to(device)\n",
    "        # self.label = label.to(device)\n",
    "        self.mask_src = mask_src.to(device)\n",
    "\n",
    "    def get(self):\n",
    "        return self.src, self.seg, self.mask_src\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dict[args.bert_model], do_lower_case=True)\n",
    "        self.bert = BertModel.from_pretrained(model_dict[args.bert_model])\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, args.vocab_label_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=args.vocab_label_size, reduction='sum')\n",
    "\n",
    "\n",
    "    def forward(self, src, seg, mask_src):\n",
    "        top_vec, _ = self.bert(input_ids=src, token_type_ids=seg, attention_mask=mask_src, return_dict=False)\n",
    "        top_vec = self.dropout(top_vec)\n",
    "        top_vec *= mask_src.unsqueeze(dim=-1).float()\n",
    "        top_vec = torch.sum(top_vec, dim=1) / mask_src.sum(dim=-1).float().unsqueeze(-1)\n",
    "        conclusion = self.linear(top_vec).squeeze()\n",
    "        return conclusion\n",
    "    \n",
    "    def get_loss(self, src, seg, label, mask_src):\n",
    "        output = self.forward(src, seg, mask_src)\n",
    "        return self.loss(output.view(-1,self.args.vocab_label_size), label.view(-1))\n",
    "\n",
    "    def predict(self, src, seg, mask_src):\n",
    "        output = self.forward(src, seg, mask_src)\n",
    "        batch_size = output.shape[0]\n",
    "        prediction = torch.argmax(output, dim=-1).data.cpu().numpy().tolist()\n",
    "        return prediction\n",
    "\n",
    "\n",
    "def prediction(dataset, model, args):\n",
    "    preds = []\n",
    "    # golds = []\n",
    "    model.eval()\n",
    "    for j in range(0, len(dataset), args.batch_size):\n",
    "        src, seg, mask_src = Batch(dataset, j, args.batch_size, args.device).get()\n",
    "        preds += model.predict(src, seg, mask_src)\n",
    "        # golds += label.cpu().data.numpy().tolist()\n",
    "    return preds\n",
    "\n",
    "def create_vocab(labels):\n",
    "    unique = np.unique(labels)\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    counter = 0\n",
    "    for word in unique:\n",
    "        label2id[word] = counter\n",
    "        id2label[counter] = word\n",
    "        counter += 1\n",
    "    return label2id, id2label\n",
    "\n",
    "def convert_label2id(label2id, labels):\n",
    "    return [label2id[x] for x in labels]\n",
    "\n",
    "def save_df(pred, id2label):\n",
    "    ids = np.arange(len(pred))\n",
    "    pred = [id2label[p] for p in pred]\n",
    "    df = pd.DataFrame()\n",
    "    df['index']=ids\n",
    "    df['label']=pred\n",
    "    df.to_csv('pred_bertW.csv', index=False)\n",
    "\n",
    "def train(args, train_dataset, dev_dataset, test_dataset, model, id2label):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    t_total = len(train_dataset) // args.batch_size * args.num_train_epochs\n",
    "    args.warmup_steps = int(0.1 * t_total)\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    logger.info(\"  Warming up = %d\", args.warmup_steps)\n",
    "    logger.info(\"  Patience  = %d\", args.patience)\n",
    "\n",
    "    # Added here for reproductibility\n",
    "    global best_model\n",
    "    set_seed(args)\n",
    "    tr_loss = 0.0\n",
    "    global_step = 1\n",
    "    best_f1_dev = 0\n",
    "    cur_patience = 0\n",
    "    for i in range(int(args.num_train_epochs)):\n",
    "        random.shuffle(train_dataset)\n",
    "        epoch_loss = 0.0\n",
    "        for j in range(0, len(train_dataset), args.batch_size):\n",
    "            src, seg, label, mask_src = Batch(train_dataset, j, args.batch_size, args.device).get()\n",
    "            model.train()\n",
    "            loss = model.get_loss(src, seg, label, mask_src)\n",
    "            loss = loss.sum()/args.batch_size\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "        logger.info(\"Finish epoch = %s, loss_epoch = %s\", i+1, epoch_loss/global_step)\n",
    "        dev_f1, _ = prediction(dev_dataset, model, args)\n",
    "        if dev_f1 > best_f1_dev:\n",
    "            best_f1_dev = dev_f1\n",
    "            _, test_pred = prediction(test_dataset, model, args)\n",
    "            save_df(test_pred, id2label)\n",
    "            #SAVE\n",
    "            cur_patience = 0\n",
    "            logger.info(\"Better, BEST F1 in DEV = %s, SAVE TEST!\", best_f1_dev)\n",
    "            best_model = model.state_dict()\n",
    "            print(best_model)\n",
    "          \n",
    "        else:\n",
    "            cur_patience += 1\n",
    "            if cur_patience == args.patience:\n",
    "                logger.info(\"Early Stopping Not Better, BEST F1 in DEV = %s\", best_f1_dev)\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"Not Better, BEST F1 in DEV = %s\", best_f1_dev)\n",
    "\n",
    "    return global_step, tr_loss / global_step, best_f1_dev\n",
    "\n",
    "\n",
    "args_parser = argparse.ArgumentParser()\n",
    "args_parser.add_argument('--bert_model', default='indobertweet', choices=['indobert', 'indobertweet'], help='select one of models')\n",
    "args_parser.add_argument('--data_path', default='./indobert_smsa/data/', help='path to all train/test/dev')\n",
    "args_parser.add_argument('--output_dir', default='/content/gdrive/MyDrive/TA_Bayu-05111940000172/Indobert/SMsA/Model/', help='path to save model')\n",
    "args_parser.add_argument('--max_token', type=int, default=128, help='maximum token allowed for 1 instance')\n",
    "args_parser.add_argument('--batch_size', type=int, default=30, help='batch size')\n",
    "args_parser.add_argument('--learning_rate', type=float, default=5e-5, help='learning rate')\n",
    "args_parser.add_argument('--weight_decay', type=int, default=0, help='weight decay')\n",
    "args_parser.add_argument('--adam_epsilon', type=float, default=1e-8, help='adam epsilon')\n",
    "args_parser.add_argument('--max_grad_norm', type=float, default=1.0)\n",
    "args_parser.add_argument('--num_train_epochs', type=int, default=20, help='total epoch')\n",
    "args_parser.add_argument('--warmup_steps', type=int, default=242, help='warmup_steps, the default value is 10% of total steps')\n",
    "args_parser.add_argument('--logging_steps', type=int, default=200, help='report stats every certain steps')\n",
    "args_parser.add_argument('--seed', type=int, default=2021)\n",
    "args_parser.add_argument('--local_rank', type=int, default=-1)\n",
    "args_parser.add_argument('--patience', type=int, default=5, help='patience for early stopping')\n",
    "args_parser.add_argument('--no_cuda', default=False)\n",
    "args = args_parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else: # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "set_seed(args)\n",
    "\n",
    "\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "bertdata = BertData(args)\n",
    "\n",
    "trainset = pd.read_csv(args.data_path+'train_preprocess.tsv', sep='\\t')\n",
    "df = pd.read_csv('data_with_replies/Final/Result/Jawa_User.csv')\n",
    "df = df.head(15)\n",
    "# devset = pd.read_csv(args.data_path+'valid_preprocess.tsv', sep='\\t')\n",
    "# testset = pd.read_csv(args.data_path+'test_preprocess_masked_label.tsv', sep='\\t')\n",
    "xtrain, ytrain = list(trainset['text']), list(trainset['label'])\n",
    "# xdev, ydev = list(devset['text']), list(devset['label'])\n",
    "# xtest, ytest = list(testset['text']), list(testset['label'])\n",
    "\n",
    "label2id, id2label = create_vocab (ytrain)\n",
    "# ytrain =  convert_label2id (label2id, ytrain)\n",
    "# ydev =  convert_label2id (label2id, ydev)\n",
    "# ytest =  convert_label2id (label2id, ytest)\n",
    "args.vocab_label_size = len(label2id)\n",
    "\n",
    "model = Model(args, device)\n",
    "best_model = model.state_dict()\n",
    "\n",
    "model.to(args.device)\n",
    "model.load_state_dict(torch.load('indobert_smsa\\model_SMSA.pt', map_location=args.device))\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "res = pd.read_csv(\"indobert_smsa/result.csv\")\n",
    "start_pos = len(res)\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "print(now)\n",
    "print(\"starting position = \", start_pos)\n",
    "\n",
    "\n",
    "try:\n",
    "    for i in range(start_pos,len(df),1000):\n",
    "        print(i)\n",
    "        batch = df.iloc[i:i+1000]\n",
    "        tweets = batch['content'].tolist()\n",
    "        index = batch['tweetID'].tolist()\n",
    "        tweets = bertdata.preprocess(tweets)\n",
    "        pred = prediction(tweets, model, args)\n",
    "\n",
    "        dataframe = pd.DataFrame({'tweetID': index, 'label': pred})\n",
    "        res = pd.concat([res, dataframe], ignore_index=True)\n",
    "        res.to_csv(\"indobert_smsa/result.csv\", index=False)\n",
    "        now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "        print(\"Saved at \",now)\n",
    "except:\n",
    "    print(\"Error at \",i)\n",
    "    dataframe = pd.DataFrame({'tweetID': index, 'label': pred})\n",
    "    res = pd.concat([res, dataframe], ignore_index=True)\n",
    "    res.to_csv(\"indobert_smsa/result.csv\", index=False)\n",
    "    now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    print(\"Saved at \",now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bangsa\n",
      "jokowi\n",
      "presiden\n",
      "1\n",
      "indonesia\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_n_keywords(text):\n",
    "    total = 0\n",
    "    for word in text.split():\n",
    "        if word in keyword['keyword']:\n",
    "            total = keyword['keyword'][word] + total\n",
    "            if keyword['keyword'][word] == 1:\n",
    "                print(word)\n",
    "    return total\n",
    "\n",
    "keywords = pd.read_csv('Utils/keyword.csv', sep=';',encoding = 'unicode_escape')\n",
    "keywords['text'] = keywords['text'].astype(str)\n",
    "keywords['text'] = keywords['text'].apply(lambda x: x.lower())\n",
    "\n",
    "keywords = keywords.drop(columns=['count'])\n",
    "keywords.set_index('text', inplace=True)\n",
    "\n",
    "keyword = keywords.to_dict()\n",
    "keyword\n",
    "\n",
    "get_n_keywords('penerus bangsa kita jokowi dodo jk presiden , nomor 1 diatas segalanya, indonesia')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'username', 'tweetID', 'content', 'likeCount', 'retweetCount',\n",
       "       'quoteCount', 'replyCount', 'label', 'weighted_sentiment',\n",
       "       'pos_sentiword', 'neg_sentiword', 'len', 'pos_inset', 'neg_inset',\n",
       "       'normalized', 'len_normalized', 'n_keywords', 'tag_owi', 'tag_prab',\n",
       "       'tag_overall'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tagged_joined.csv', sep=';')\n",
    "df = df.dropna()\n",
    "df = df[df['tag_overall'] != 5]\n",
    "df = df[df['tag_overall'] != 4]\n",
    "df['tag_overall'].value_counts()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.54      0.15      0.23       350\n",
      "         neu       0.32      0.68      0.44       242\n",
      "         pos       0.27      0.24      0.26       175\n",
      "\n",
      "    accuracy                           0.34       767\n",
      "   macro avg       0.38      0.36      0.31       767\n",
      "weighted avg       0.41      0.34      0.30       767\n",
      "\n",
      "[[ 52 241  57]\n",
      " [ 22 165  55]\n",
      " [ 23 110  42]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "pos_sentiword = df['pos_sentiword'].tolist()\n",
    "neg_sentiword = df['neg_sentiword'].tolist()\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(pos_sentiword)):\n",
    "    delta = pos_sentiword[i] - neg_sentiword[i]\n",
    "    if delta > 0.5:\n",
    "        y_pred.append(3)\n",
    "    elif delta < -0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(2)\n",
    "\n",
    "y_true = df['tag_overall'].tolist()\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'posSentiword'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Bayu Adjie Sidharta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Bayu Adjie Sidharta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Bayu Adjie Sidharta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'posSentiword'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pos_sentiword \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mposSentiword\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m      6\u001b[0m neg_sentiword \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mnegSentiword\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m      7\u001b[0m y_true \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtag_overall\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Bayu Adjie Sidharta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Bayu Adjie Sidharta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'posSentiword'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "pos_sentiword = df['posSentiword'].tolist()\n",
    "neg_sentiword = df['negSentiword'].tolist()\n",
    "y_true = df['tag_overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1, 192, 384, 575, 767], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(1, len(df), 5, dtype = np.int64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25;0.3224115334207077 SSS\n",
      "0.5;0.33551769331585846 SSS\n",
      "0.75;0.32765399737876805 SSS\n",
      "1.0;0.3237221494102228 SSS\n",
      "1.25;0.3302752293577982 SSS\n",
      "1.5;0.3328964613368283 SSS\n",
      "1.75;0.3197903014416776 SSS\n",
      "2.0;0.3197903014416776 SSS\n",
      "2.25;0.31847968545216254 SSS\n",
      "2.5;0.31716906946264745 SSS\n",
      "2.75;0.3132372214941022 SSS\n",
      "3.0;0.3132372214941022 SSS\n",
      "3.25;0.3132372214941022 SSS\n",
      "3.5;0.3132372214941022 SSS\n",
      "3.75;0.3119266055045872 SSS\n",
      "4.0;0.3119266055045872 SSS\n",
      "4.25;0.3119266055045872 SSS\n",
      "4.5;0.3119266055045872 SSS\n",
      "4.75;0.3119266055045872 SSS\n"
     ]
    }
   ],
   "source": [
    "max_acc = 0\n",
    "best_treshold = 0\n",
    "\n",
    "for i in range(1, 20):\n",
    "    treshold = i / 4\n",
    "    y_pred = []\n",
    "\n",
    "\n",
    "    for x in range(len(pos_sentiword)):\n",
    "        delta = pos_sentiword[x] - neg_sentiword[x]\n",
    "        if delta > treshold:\n",
    "            y_pred.append(3)\n",
    "        elif delta < -treshold:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(2)\n",
    "    accuracy = np.sum(np.array(y_true) == np.array(y_pred)) / len(y_true)\n",
    "    print(str(treshold) + \";\" + str(accuracy) + \" SSS\")\n",
    "    # if accuracy > max_acc:\n",
    "    #     max_acc = accuracy\n",
    "    #     best_treshold = treshold\n",
    "    #     print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "    #     print(confusion_matrix(y_true, y_pred))\n",
    "    #     print(\"Max Accuracy = \", max_acc, \"Best Treshold = \", best_treshold)\n",
    "    #     print(\"=====================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_inset = df['pos_inset'].tolist()\n",
    "neg_inset = df['neg_inset'].tolist()\n",
    "y_true = df['tag_overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1;0.4198174706649283 SSS\n",
      "2;0.4315514993481095 SSS\n",
      "3;0.4302477183833116 SSS\n",
      "4;0.4315514993481095 SSS\n",
      "5;0.423728813559322 SSS\n",
      "6;0.42633637548891784 SSS\n",
      "7;0.40547588005215124 SSS\n",
      "8;0.39374185136897 SSS\n",
      "9;0.3833116036505867 SSS\n",
      "10;0.37809647979139505 SSS\n",
      "11;0.37027379400260757 SSS\n",
      "12;0.3650586701434159 SSS\n",
      "13;0.37157757496740546 SSS\n",
      "14;0.37027379400260757 SSS\n",
      "15;0.36766623207301175 SSS\n",
      "16;0.3663624511082138 SSS\n",
      "17;0.35723598435462844 SSS\n",
      "18;0.35071707953063885 SSS\n",
      "19;0.34028683181225555 SSS\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "max_acc = 0\n",
    "best_treshold = 0\n",
    "\n",
    "for i in range(1, 20, 1):\n",
    "    treshold = i \n",
    "    y_pred = []\n",
    "\n",
    "\n",
    "    for x in range(len(pos_inset)):\n",
    "        delta = pos_inset[x] + neg_inset[x]\n",
    "        if delta > treshold:\n",
    "            y_pred.append(3)\n",
    "        elif delta < - treshold:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(2)\n",
    "    # print(\"Treshold = \", treshold)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # print(\"Accuracy = \", accuracy)\n",
    "    print(str(treshold) + \";\" + str(accuracy) + \" SSS\")\n",
    "    if accuracy > max_acc:\n",
    "        max_acc = accuracy\n",
    "        best_treshold = treshold\n",
    "        # print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "        # print(confusion_matrix(y_true, y_pred))\n",
    "        # print(\"Max Accuracy = \", max_acc, \"Best Treshold = \", best_treshold)\n",
    "        # print(\"=====================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_inset = df['pos_inset'].tolist()\n",
    "neg_inset = df['neg_inset'].tolist()\n",
    "y_true = df['tag_overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.54      0.50      0.52       350\n",
      "         neu       0.43      0.23      0.30       242\n",
      "         pos       0.32      0.57      0.41       175\n",
      "\n",
      "    accuracy                           0.43       767\n",
      "   macro avg       0.43      0.43      0.41       767\n",
      "weighted avg       0.46      0.43      0.43       767\n",
      "\n",
      "[[175  45 130]\n",
      " [100  56  86]\n",
      " [ 47  28 100]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "for i in range(len(pos_inset)):\n",
    "    delta = pos_inset[i] + neg_inset[i]\n",
    "    if delta > 2:\n",
    "        y_pred.append(3)\n",
    "    elif delta < -2:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(2)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.55      0.41      0.47       350\n",
      "         neu       0.40      0.40      0.40       242\n",
      "         pos       0.34      0.50      0.40       175\n",
      "\n",
      "    accuracy                           0.43       767\n",
      "   macro avg       0.43      0.44      0.43       767\n",
      "weighted avg       0.46      0.43      0.44       767\n",
      "\n",
      "[[145 100 105]\n",
      " [ 77  98  67]\n",
      " [ 40  47  88]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "for i in range(len(pos_inset)):\n",
    "    delta = pos_inset[i] + neg_inset[i]\n",
    "    if delta > 4:\n",
    "        y_pred.append(3)\n",
    "    elif delta < -4:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(2)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    767.000000\n",
       "mean      13.859192\n",
       "std        9.459878\n",
       "min        0.000000\n",
       "25%        7.000000\n",
       "50%       12.000000\n",
       "75%       19.000000\n",
       "max       51.000000\n",
       "Name: pos_inset, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pos_inset'].describe()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    767.000000\n",
       "mean     -14.186441\n",
       "std        9.950159\n",
       "min      -82.000000\n",
       "25%      -20.000000\n",
       "50%      -13.000000\n",
       "75%       -7.000000\n",
       "max        0.000000\n",
       "Name: neg_inset, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['neg_inset'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n",
      "196\n",
      "199\n",
      "202\n",
      "178\n",
      "176\n",
      "212\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "pos = df['pos_inset'].quantile([0, 0.25, 0.5, 0.75, 1]).tolist()\n",
    "neg = df['neg_inset'].quantile([0, 0.25, 0.5, 0.75, 1]).tolist()\n",
    "\n",
    "\n",
    "for i in range(len(pos) - 1) :\n",
    "    # print number of values in between each quantile\n",
    "    print(df[(df['pos_inset'] >= pos[i]) & (df['pos_inset'] < pos[i+1])].shape[0])\n",
    "          \n",
    "for i in range(len(neg) - 1) :\n",
    "    # print number of values in between each quantile\n",
    "    print(df[(df['neg_inset'] >= neg[i]) & (df['neg_inset'] < neg[i+1])].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.56      0.23      0.32       349\n",
      "         neu       0.34      0.69      0.46       239\n",
      "         pos       0.31      0.25      0.27       175\n",
      "\n",
      "    accuracy                           0.38       763\n",
      "   macro avg       0.41      0.39      0.35       763\n",
      "weighted avg       0.44      0.38      0.35       763\n",
      "\n",
      "[[ 79 208  62]\n",
      " [ 40 166  33]\n",
      " [ 21 111  43]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "for i in range(len(pos_inset)):\n",
    "    delta = pos_inset[i] + neg_inset[i]\n",
    "    if delta > 10:\n",
    "        y_pred.append(3)\n",
    "    elif delta < -10:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(2)\n",
    "\n",
    "y_true = df['tag_overall'].tolist()\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    763.000000\n",
       "mean       0.510976\n",
       "std        0.522797\n",
       "min        0.000000\n",
       "25%        0.125000\n",
       "50%        0.375000\n",
       "75%        0.750000\n",
       "max        3.875000\n",
       "Name: posSentiword, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['posSentiword'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    763.000000\n",
       "mean       0.422182\n",
       "std        0.599809\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.250000\n",
       "75%        0.625000\n",
       "max        6.750000\n",
       "Name: negSentiword, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['negSentiword'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = df['pos_sentiword'].quantile([0, 0.25, 0.5, 0.75, 1]).tolist()\n",
    "neg = df['neg_sentiword'].quantile([0, 0.25, 0.5, 0.75, 1]).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.25, 0.625, 6.75]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "166\n",
      "215\n",
      "222\n",
      "0\n",
      "353\n",
      "203\n",
      "210\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pos) - 1) :\n",
    "    # print number of values in between each quantile\n",
    "    print(df[(df['pos_sentiword'] >= pos[i]) & (df['pos_sentiword'] < pos[i+1])].shape[0])\n",
    "          \n",
    "for i in range(len(neg) - 1) :\n",
    "    # print number of values in between each quantile\n",
    "    print(df[(df['neg_sentiword'] >= neg[i]) & (df['neg_sentiword'] < neg[i+1])].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449\n",
      "241\n",
      "211\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(neg)):\n",
    "    # print number of data in each quantile range\n",
    "    if i == len(neg) - 1:\n",
    "        print(df[(df['neg_sentiword'] >= neg[i])].shape[0])\n",
    "    else:  \n",
    "        print(df[(df['neg_sentiword'] >= neg[i]) & (df['neg_sentiword'] <= neg[i+1])].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweetID</th>\n",
       "      <th>content</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>quotes</th>\n",
       "      <th>replies</th>\n",
       "      <th>BERTlabel</th>\n",
       "      <th>weightedBERTlabel</th>\n",
       "      <th>posSentiword</th>\n",
       "      <th>negSentiword</th>\n",
       "      <th>posInset</th>\n",
       "      <th>negInset</th>\n",
       "      <th>n_keywords</th>\n",
       "      <th>tag_overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Singhasari1982</td>\n",
       "      <td>1074194773559304194</td>\n",
       "      <td>@marierteman @prabowo PRABOWO AJA\\nPRABOWO AJA...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NurSyahbana9</td>\n",
       "      <td>1067997009859141632</td>\n",
       "      <td>ULAMA PEWARIS NABI\\nApa bedanya antara Ulama p...</td>\n",
       "      <td>87</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>-67</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42</td>\n",
       "      <td>-2</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>didienAZHAR</td>\n",
       "      <td>1115833133964910597</td>\n",
       "      <td>Lima kueri tertinggi di antaranya yakni kampan...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fariji_lacak</td>\n",
       "      <td>1052383624698314753</td>\n",
       "      <td>@YCH7168 @mochamadarip @NaneDianti @GunRomli *...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.125</td>\n",
       "      <td>37</td>\n",
       "      <td>-19</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fariji_lacak</td>\n",
       "      <td>1052382472174886912</td>\n",
       "      <td>*Hanya di-Era Jokowi, Ada Ulama KHUSUS*\\n1.Ula...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.125</td>\n",
       "      <td>37</td>\n",
       "      <td>-19</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        username              tweetID   \n",
       "0   0  Singhasari1982  1074194773559304194  \\\n",
       "1   1    NurSyahbana9  1067997009859141632   \n",
       "2   2     didienAZHAR  1115833133964910597   \n",
       "3   3    fariji_lacak  1052383624698314753   \n",
       "4   4    fariji_lacak  1052382472174886912   \n",
       "\n",
       "                                             content  likes  retweets  quotes   \n",
       "0  @marierteman @prabowo PRABOWO AJA\\nPRABOWO AJA...      2         0       0  \\\n",
       "1  ULAMA PEWARIS NABI\\nApa bedanya antara Ulama p...     87        24       2   \n",
       "2  Lima kueri tertinggi di antaranya yakni kampan...      1         1       0   \n",
       "3  @YCH7168 @mochamadarip @NaneDianti @GunRomli *...      4         2       0   \n",
       "4  *Hanya di-Era Jokowi, Ada Ulama KHUSUS*\\n1.Ula...      3         0       0   \n",
       "\n",
       "   replies  BERTlabel  weightedBERTlabel  posSentiword  negSentiword   \n",
       "0        0         -1                 -1         0.000         0.000  \\\n",
       "1       13         -1                -67         0.000         0.000   \n",
       "2        0          0                  0         0.125         0.000   \n",
       "3        0         -1                 -4         0.500         0.125   \n",
       "4        1         -1                 -1         0.500         0.125   \n",
       "\n",
       "   posInset  negInset  n_keywords  tag_overall  \n",
       "0         0         0          23          3.0  \n",
       "1        42        -2          19          1.0  \n",
       "2        18         0          18          3.0  \n",
       "3        37       -19          17          1.0  \n",
       "4        37       -19          17          1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "max_acc = 0\n",
    "best_treshold = 0\n",
    "\n",
    "for i in range(1, 20, 1):\n",
    "    treshold = i \n",
    "    y_pred = []\n",
    "\n",
    "\n",
    "    for x in range(len(pos_inset)):\n",
    "        delta = pos_inset[x] + neg_inset[x]\n",
    "        if delta > treshold:\n",
    "            y_pred.append(3)\n",
    "        elif delta < - treshold:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(2)\n",
    "    # print(\"Treshold = \", treshold)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # print(\"Accuracy = \", accuracy)\n",
    "    if accuracy > max_acc:\n",
    "        max_acc = accuracy\n",
    "        best_treshold = treshold\n",
    "        print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        print(\"Max Accuracy = \", max_acc, \"Best Treshold = \", best_treshold)\n",
    "        print(\"=====================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_barasa = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.54      0.15      0.23       350\n",
      "         neu       0.32      0.68      0.44       242\n",
      "         pos       0.27      0.24      0.26       175\n",
      "\n",
      "    accuracy                           0.34       767\n",
      "   macro avg       0.38      0.36      0.31       767\n",
      "weighted avg       0.41      0.34      0.30       767\n",
      "\n",
      "[[ 52 241  57]\n",
      " [ 22 165  55]\n",
      " [ 23 110  42]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "pos_sentiword = df['pos_sentiword'].tolist()\n",
    "neg_sentiword = df['neg_sentiword'].tolist()\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(pos_sentiword)):\n",
    "    delta = pos_sentiword[i] - neg_sentiword[i]\n",
    "    if delta > 0.5:\n",
    "        y_pred.append(3)\n",
    "    elif delta < -0.5:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(2)\n",
    "\n",
    "y_true = df['tag_overall'].tolist()\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_barasa['Barasa'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_barasa = wrong_answer_barasa[wrong_answer_barasa['tag_overall'] != wrong_answer_barasa['Barasa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_barasa.to_csv('wrong_answer_barasa.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpr korupsi pdip turoechan asy ari dprd izedrik emir moeis dpr agus chondro prayitno dpr max moein anggota dpr rusman lumbantoruan dpr poltak sitorus dpr panda nababan dpr engelina patiasina dpr m iqbal dpr budiningsih dpr effri tongas dpr mariani dpr\n",
      "Found korupsi with pos 0.125 and neg 0.125\n",
      "Found emir with pos 0.0 and neg 0.0\n",
      "Found agus with pos 0.0 and neg 0.125\n",
      "Found anggota with pos 0.0 and neg 0.0\n",
      "Found panda with pos 0.0 and neg 0.0\n",
      "Found 5 out of 41 tokens\n",
      "korupsi, emir, agus, anggota, panda\n",
      "Unique tokens found: 5\n",
      "0.125 0.25\n",
      "Netral\n"
     ]
    }
   ],
   "source": [
    "with open('teks.txt', 'r') as f:\n",
    "    teks = f.read()\n",
    "\n",
    "print(\" \".join(normalize.normalize(teks)))\n",
    "pos, neg = barasa.calculate_sentiment(normalize.normalize(teks))\n",
    "print(np.round(np.sum(pos), 4), np.round(np.sum(neg), 4))\n",
    "pos = np.round(np.sum(pos), 4)\n",
    "neg = np.round(np.sum(neg), 4)\n",
    "if pos - neg > 0.5:\n",
    "    print(\"Positif\")\n",
    "elif pos - neg < -0.5:\n",
    "    print(\"Negatif\")\n",
    "else:\n",
    "    print(\"Netral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_inset = df\n",
    "wrong_answer_inset['inset'] =  y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_inset = wrong_answer_inset[wrong_answer_inset['tag_overall'] != wrong_answer_inset['inset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweetID</th>\n",
       "      <th>content</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>label</th>\n",
       "      <th>weighted_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_inset</th>\n",
       "      <th>neg_inset</th>\n",
       "      <th>normalized</th>\n",
       "      <th>len_normalized</th>\n",
       "      <th>n_keywords</th>\n",
       "      <th>tag_owi</th>\n",
       "      <th>tag_prab</th>\n",
       "      <th>tag_overall</th>\n",
       "      <th>Barasa</th>\n",
       "      <th>inset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Singhasari1982</td>\n",
       "      <td>1074194773559304194</td>\n",
       "      <td>@marierteman @prabowo PRABOWO AJA\\nPRABOWO AJA...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>prabowo prabowo prabowo prabowo prabowo prabow...</td>\n",
       "      <td>183</td>\n",
       "      <td>23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NurSyahbana9</td>\n",
       "      <td>1067997009859141632</td>\n",
       "      <td>ULAMA PEWARIS NABI\\nApa bedanya antara Ulama p...</td>\n",
       "      <td>87</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-67.5</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>-2</td>\n",
       "      <td>ulama pewaris nabi beda ulama pendukung presid...</td>\n",
       "      <td>195</td>\n",
       "      <td>19</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fariji_lacak</td>\n",
       "      <td>1052383624698314753</td>\n",
       "      <td>@YCH7168 @mochamadarip @NaneDianti @GunRomli *...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>-19</td>\n",
       "      <td>era jokowi ulama ulama instan ulama yutub meds...</td>\n",
       "      <td>172</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>fariji_lacak</td>\n",
       "      <td>1052382472174886912</td>\n",
       "      <td>*Hanya di-Era Jokowi, Ada Ulama KHUSUS*\\n1.Ula...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>-19</td>\n",
       "      <td>era jokowi ulama ulama instan ulama yutub meds...</td>\n",
       "      <td>172</td>\n",
       "      <td>17</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Ars_sand</td>\n",
       "      <td>1097878146861346816</td>\n",
       "      <td>@putrabanten80 @ZAEffendy @RamliRizal Kalau ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>-10</td>\n",
       "      <td>kerja kerja kerja mikir kerja kerja kerja utan...</td>\n",
       "      <td>133</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>762</td>\n",
       "      <td>NurSyahbana9</td>\n",
       "      <td>1108712236048998401</td>\n",
       "      <td>Komitmen NAHDHATUL ULAMA untuk menjaga dan mem...</td>\n",
       "      <td>298</td>\n",
       "      <td>178</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>-9</td>\n",
       "      <td>komitmen nahdhatul ulama menjaga membentengi n...</td>\n",
       "      <td>209</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>764</td>\n",
       "      <td>PRFMnews</td>\n",
       "      <td>1099676060600258562</td>\n",
       "      <td>#DiskusiPRFM KPK menyambut baik langkah KPU ya...</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>-17</td>\n",
       "      <td>kpk menyambut langkah kpu mengumumkan tambahan...</td>\n",
       "      <td>209</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>765</td>\n",
       "      <td>erna_st</td>\n",
       "      <td>1059458181275955200</td>\n",
       "      <td>Tim Kmpanye Nasional Jokowi-Ma’ruf, yg dipimpi...</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-14</td>\n",
       "      <td>tim kmpanye nasional jokowi ruf dipimpin erick...</td>\n",
       "      <td>209</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>767</td>\n",
       "      <td>hendriabidin2</td>\n",
       "      <td>1113122672416743425</td>\n",
       "      <td>@tody_mt1 @cahw_i @McKayAudy @J_Aryoko @_SEKNA...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>-12</td>\n",
       "      <td>tong penurunan zaman sby dibantu blt subsidi b...</td>\n",
       "      <td>208</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>770</td>\n",
       "      <td>kpusukoharjo</td>\n",
       "      <td>1067704568354549760</td>\n",
       "      <td>Proliman Sukoharjo menjadi lokasi sosialisasi ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>-24</td>\n",
       "      <td>proliman sukoharjo lokasi sosialisasi pemilu w...</td>\n",
       "      <td>207</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>436 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id        username              tweetID   \n",
       "0      0  Singhasari1982  1074194773559304194  \\\n",
       "1      1    NurSyahbana9  1067997009859141632   \n",
       "3      3    fariji_lacak  1052383624698314753   \n",
       "4      4    fariji_lacak  1052382472174886912   \n",
       "5      5        Ars_sand  1097878146861346816   \n",
       "..   ...             ...                  ...   \n",
       "757  762    NurSyahbana9  1108712236048998401   \n",
       "759  764        PRFMnews  1099676060600258562   \n",
       "760  765         erna_st  1059458181275955200   \n",
       "762  767   hendriabidin2  1113122672416743425   \n",
       "765  770    kpusukoharjo  1067704568354549760   \n",
       "\n",
       "                                               content  likeCount   \n",
       "0    @marierteman @prabowo PRABOWO AJA\\nPRABOWO AJA...          2  \\\n",
       "1    ULAMA PEWARIS NABI\\nApa bedanya antara Ulama p...         87   \n",
       "3    @YCH7168 @mochamadarip @NaneDianti @GunRomli *...          4   \n",
       "4    *Hanya di-Era Jokowi, Ada Ulama KHUSUS*\\n1.Ula...          3   \n",
       "5    @putrabanten80 @ZAEffendy @RamliRizal Kalau ha...          1   \n",
       "..                                                 ...        ...   \n",
       "757  Komitmen NAHDHATUL ULAMA untuk menjaga dan mem...        298   \n",
       "759  #DiskusiPRFM KPK menyambut baik langkah KPU ya...         23   \n",
       "760  Tim Kmpanye Nasional Jokowi-Ma’ruf, yg dipimpi...         15   \n",
       "762  @tody_mt1 @cahw_i @McKayAudy @J_Aryoko @_SEKNA...          1   \n",
       "765  Proliman Sukoharjo menjadi lokasi sosialisasi ...          5   \n",
       "\n",
       "     retweetCount  quoteCount  replyCount  label  weighted_sentiment  ...   \n",
       "0               0           0           0   -1.0                -1.0  ...  \\\n",
       "1              24           2          13   -1.0               -67.5  ...   \n",
       "3               2           0           0   -1.0                -4.0  ...   \n",
       "4               0           0           1   -1.0                -1.5  ...   \n",
       "5               0           0           0   -1.0                -0.5  ...   \n",
       "..            ...         ...         ...    ...                 ...  ...   \n",
       "757           178          24          43    1.0               327.0  ...   \n",
       "759             5           1          14    0.0                 0.0  ...   \n",
       "760            13           3          13    0.0                 0.0  ...   \n",
       "762             0           0           2   -1.0                -0.5  ...   \n",
       "765             2           0           0    0.0                 0.0  ...   \n",
       "\n",
       "     pos_inset  neg_inset                                         normalized   \n",
       "0            0          0  prabowo prabowo prabowo prabowo prabowo prabow...  \\\n",
       "1           42         -2  ulama pewaris nabi beda ulama pendukung presid...   \n",
       "3           37        -19  era jokowi ulama ulama instan ulama yutub meds...   \n",
       "4           37        -19  era jokowi ulama ulama instan ulama yutub meds...   \n",
       "5           37        -10  kerja kerja kerja mikir kerja kerja kerja utan...   \n",
       "..         ...        ...                                                ...   \n",
       "757         12         -9  komitmen nahdhatul ulama menjaga membentengi n...   \n",
       "759         26        -17  kpk menyambut langkah kpu mengumumkan tambahan...   \n",
       "760          3        -14  tim kmpanye nasional jokowi ruf dipimpin erick...   \n",
       "762         16        -12  tong penurunan zaman sby dibantu blt subsidi b...   \n",
       "765          4        -24  proliman sukoharjo lokasi sosialisasi pemilu w...   \n",
       "\n",
       "     len_normalized  n_keywords tag_owi  tag_prab  tag_overall  Barasa  inset  \n",
       "0               183          23     4.0       3.0          3.0       2      2  \n",
       "1               195          19     3.0       1.0          1.0       2      3  \n",
       "3               172          17     2.0       4.0          1.0       2      3  \n",
       "4               172          17     2.0       4.0          1.0       2      3  \n",
       "5               133          17     1.0       2.0          1.0       3      3  \n",
       "..              ...         ...     ...       ...          ...     ...    ...  \n",
       "757             209           8     4.0       4.0          3.0       2      2  \n",
       "759             209           8     4.0       4.0          2.0       2      3  \n",
       "760             209           8     4.0       4.0          2.0       2      1  \n",
       "762             208           8     4.0       4.0          1.0       3      2  \n",
       "765             207           8     4.0       4.0          2.0       2      1  \n",
       "\n",
       "[436 rows x 23 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_answer_inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpr korupsi pdip turoechan asy ari dprd izedrik emir moeis dpr agus chondro prayitno dpr max moein anggota dpr rusman lumbantoruan dpr poltak sitorus dpr panda nababan dpr engelina patiasina dpr m iqbal dpr budiningsih dpr effri tongas dpr mariani dpr\n",
      "Negative Unigram korupsi with score -4\n",
      "Positive Unigram emir with score 4\n",
      "Negative Unigram anggota with score -3\n",
      "Found 1 out of 41 tokens\n",
      "emir\n",
      "Unique tokens found: 1\n",
      "4.0 -7.0\n",
      "Netral\n"
     ]
    }
   ],
   "source": [
    "with open('teks.txt', 'r') as f:\n",
    "    teks = f.read()\n",
    "\n",
    "print(\" \".join(normalize.normalize(teks)))\n",
    "pos, neg = insett.calculate_inset_score(\" \".join(normalize.normalize(teks)))\n",
    "print(np.round(np.mean(pos), 4), np.round(np.mean(neg), 4))\n",
    "if pos + neg > 4: \n",
    "    print(\"Positif\")\n",
    "elif pos + neg < -4:\n",
    "    print(\"Negatif\")\n",
    "else:\n",
    "    print(\"Netral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indobert = df['label'].tolist()\n",
    "y_true = df['tag_overall'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_indobert = [int(i + 2) for i in label_indobert]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.71      0.94      0.81       350\n",
      "         neu       0.80      0.53      0.64       242\n",
      "         pos       0.73      0.62      0.67       175\n",
      "\n",
      "    accuracy                           0.74       767\n",
      "   macro avg       0.75      0.69      0.71       767\n",
      "weighted avg       0.75      0.74      0.72       767\n",
      "\n",
      "[[328  11  11]\n",
      " [ 86 128  28]\n",
      " [ 46  21 108]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, label_indobert, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, label_indobert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_indobert = df\n",
    "wrong_answer_indobert['indobert'] = label_indobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_indobert = wrong_answer_indobert[wrong_answer_indobert['tag_overall'] != wrong_answer_indobert['indobert']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_answer_indobert.to_csv('wrong_answer_indobert.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
