{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nlp_id.tokenizer import Tokenizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from nlp_id.stopword import StopWord \n",
    "from nltk.corpus import stopwords\n",
    "import words as w\n",
    "\n",
    "class normalizer():\n",
    "    def __init__(self):\n",
    "        nltk.download('stopwords')\n",
    "        stopwords_sastrawi = StopWordRemoverFactory()\n",
    "        stopwords_nlpid = StopWord() \n",
    "        stopwords_nltk = stopwords.words('indonesian')\n",
    "        stopwords_github = list(np.array(pd.read_csv(\"Utils/stopwords.txt\", header=None).values).squeeze())\n",
    "        more_stopword = w.custom_stopwords\n",
    "        data_stopword = stopwords_sastrawi.get_stop_words() + stopwords_nlpid.get_stopword() + stopwords_github + stopwords_nltk + more_stopword \n",
    "        data_stopword = list(set(data_stopword))\n",
    "\n",
    "        # Only use 'rt' as stopwords\n",
    "        data_stopword = list(set(data_stopword))\n",
    "\n",
    "        # Combine slang dictionary\n",
    "        import json\n",
    "        with open('Utils/slang.txt') as f:\n",
    "            data = f.read()\n",
    "        data_slang = json.loads(data) \n",
    "\n",
    "        with open('Utils/sinonim.txt') as f:\n",
    "            data = f.readlines()\n",
    "        for line in data:\n",
    "            word = line.split('=')\n",
    "            data_slang[word[0].strip()] = word[1].strip()\n",
    "\n",
    "        # print(data_slang)\n",
    "        more_dict = w.custom_dict\n",
    "        data_slang.update(more_dict)\n",
    "\n",
    "        self.stopwords, self.slang = data_stopword, data_slang\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "    def normalize(self,text):\n",
    "        text = text.lower()\n",
    "  \n",
    "        # Change HTML entities\n",
    "        text = text.replace('&amp;', 'dan')\n",
    "        text = text.replace('&gt;', 'lebih dari')\n",
    "        text = text.replace('&lt;', 'kurang dari')\n",
    "        \n",
    "        # Remove url\n",
    "        text = re.sub(r'http\\S+', 'httpurl', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        \n",
    "        # Remove hashtags\n",
    "        text = re.sub(r'#\\w+', ' ', text)\n",
    "        \n",
    "        # Replace @mentions with 'user'\n",
    "        text = re.sub(r'@\\w+', 'user', text)\n",
    "\n",
    "        # Remove non-letter characters\n",
    "        text = re.sub('[^a-zA-z]', ' ', text)\n",
    "\n",
    "        # Remove excess space\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.strip()\n",
    "\n",
    "        result = []\n",
    "        word_token = self.tokenizer.tokenize(text) # Tokenize words\n",
    "        for word in word_token:\n",
    "            word = word.strip().lower() # Case Folding to Lower Case\n",
    "            if word in self.slang:\n",
    "                word = self.slang[word]\n",
    "            if word not in self.stopwords: # Stopwords removal\n",
    "                result.append(word)\n",
    "            else:\n",
    "                continue\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Bayu Adjie\n",
      "[nltk_data]     Sidharta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coba', 'bayangkan', 'ketua', 'mk', 'ketua', 'panglima', 'tni', 'jaksa', 'agung', 'ketua', 'kpk', 'kepala', 'bin', 'kapolri', 'alasan', 'cawe', 'memenangkan', 'capres', 'cawapres', 'pemilu', 'maksud']\n"
     ]
    }
   ],
   "source": [
    "normalize = normalizer()\n",
    "test_text = \"Luar biasa! Coba kita bayangkan apa yg bakal terjadi jika Ketua MK, Ketua MA, Panglima TNI, Jaksa Agung, Ketua KPK, Kepala BIN, dan Kapolri juga dgn menggunakan alasan yg sama ikut cawe2 dlm memenangkan Capres-Cawapres tertentu dlm Pemilu 2024? Itukah maksudnya?#RakyatMonitor#\"\n",
    "print(normalize.normalize(test_text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.corpus.reader import WordNetError\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nlp_id.tokenizer import Tokenizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from nlp_id.stopword import StopWord \n",
    "from nltk.corpus import stopwords\n",
    "import words as w\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "\n",
    "class SentiSynset:\n",
    "    def __init__(self, pos_score, neg_score, synset):\n",
    "        self._pos_score = pos_score\n",
    "        self._neg_score = neg_score\n",
    "        self._obj_score = 1.0 - (self._pos_score + self._neg_score)\n",
    "        self.synset = synset\n",
    "\n",
    "\n",
    "    def pos_score(self):\n",
    "        return self._pos_score\n",
    "\n",
    "\n",
    "    def neg_score(self):\n",
    "        return self._neg_score\n",
    "\n",
    "\n",
    "    def obj_score(self):\n",
    "        return self._obj_score\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Prints just the Pos/Neg scores for now.\"\"\"\n",
    "        s = \"<\"\n",
    "        s += self.synset.name() + \": \"\n",
    "        s += \"PosScore=%s \" % self._pos_score\n",
    "        s += \"NegScore=%s\" % self._neg_score\n",
    "        s += \">\"\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Senti\" + repr(self.synset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomSentiWordNet(object):\n",
    "    def __init__(self):\n",
    "        with open(\"Utils/barasa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        # create empty 2d dict\n",
    "        synsets = {}\n",
    "        id_dict = {}\n",
    "        for line in lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 6:\n",
    "                continue\n",
    "            synset_id = parts[0]\n",
    "\n",
    "            if synset_id not in synsets:\n",
    "                synsets[synset_id] = {}\n",
    "            \n",
    "            synset = {}\n",
    "            id, lang, goodness, lemma, pos, neg = parts\n",
    "            pos = float(pos)\n",
    "            neg = float(neg)\n",
    "            synsets[synset_id][lemma] = (pos, neg, 1 - (pos + neg))\n",
    "            id_dict[lemma] = synset_id\n",
    "\n",
    "        self.lemma_dict = id_dict\n",
    "        self.synsets = synsets\n",
    "        self.not_found = {}\n",
    "    \n",
    "    def _get_synset(self, synset_id):\n",
    "        # helper function to map synset_id to synset\n",
    "        synsets = self.synsets[synset_id]\n",
    "        return synsets\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _get_pos_file(self, pos):\n",
    "        # helper function to map WordNet POS tags to file names\n",
    "        if pos == 'n':\n",
    "            return 'noun'\n",
    "        elif pos == 'v':\n",
    "            return 'verb'\n",
    "        elif pos == 'a' or pos == 's':\n",
    "            return 'adj'\n",
    "        elif pos == 'r':\n",
    "            return 'adv'\n",
    "        else:\n",
    "            raise WordNetError('Unknown POS tag: {}'.format(pos))\n",
    "    \n",
    "    \n",
    "    def senti_synset(self, synset_id):\n",
    "        pos_score,neg_score,obj_score = self.synsets[synset_id]\n",
    "        synset = self._get_synset(synset_id)\n",
    "        return SentiSynset(synset, pos_score, neg_score)\n",
    "    \n",
    "    def calculate_sentiment(self,tokens):\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for token in tokens:\n",
    "            if token not in self.lemma_dict:\n",
    "                self.not_found[token] = self.not_found.get(token, 0) + 1\n",
    "                continue\n",
    "            synsets = self.synsets[self.lemma_dict[token]][token]\n",
    "            pos_score, neg_score, obj_score = synsets\n",
    "            pos.append(pos_score)\n",
    "            neg.append(neg_score)\n",
    "        return pos, neg\n",
    "    \n",
    "    def get_not_found(self):\n",
    "        return self.not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.125, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125])\n"
     ]
    }
   ],
   "source": [
    "barasa = CustomSentiWordNet()\n",
    "print(barasa.calculate_sentiment(normalize.normalize(test_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mk': 1,\n",
       " 'tni': 1,\n",
       " 'kpk': 1,\n",
       " 'bin': 1,\n",
       " 'kapolri': 1,\n",
       " 'cawe': 1,\n",
       " 'capres': 1,\n",
       " 'cawapres': 1,\n",
       " 'pemilu': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barasa.get_not_found()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "def read_inset(path):\n",
    "    sentiments = {}\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        word, sentiment = line.split('\\t')\n",
    "        sentiments[word] = int(sentiment)\n",
    "    print(len(sentiments))\n",
    "    return sentiments\n",
    "\n",
    "def print_n_grams(unigrams, bigrams, trigrams):\n",
    "    print('Unigrams: ', ', '.join(unigrams))\n",
    "    print('Bigrams: ', ', '.join(bigrams))\n",
    "    print('Trigrams: ', ', '.join(trigrams))\n",
    "\n",
    "    \n",
    "\n",
    "class inSet():\n",
    "    def __init__(self, verbose = False):\n",
    "        self.pos = read_inset('Utils/Inset/positive.tsv')\n",
    "        self.neg = read_inset('Utils/Inset/negative.tsv')\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def delete_word_from_text(self, text, word):\n",
    "        text = text.replace(word, '', 1)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def calculate_n_gram(self, text):\n",
    "        unigrams = ngrams(text.split(), 1)\n",
    "        bigrams = ngrams(text.split(), 2)\n",
    "        trigrams = ngrams(text.split(), 3)\n",
    "\n",
    "        unigrams = [' '.join(grams) for grams in unigrams]\n",
    "        bigrams = [' '.join(grams) for grams in bigrams]\n",
    "        trigrams = [' '.join(grams) for grams in trigrams]\n",
    "\n",
    "        return unigrams, bigrams, trigrams\n",
    "    \n",
    "    def recalculate_n_grams(self, text, word):\n",
    "        text = self.delete_word_from_text(text, word)\n",
    "        unigrams, bigrams, trigrams = self.calculate_n_gram(text)\n",
    "        if self.verbose:\n",
    "            print_n_grams(unigrams, bigrams, trigrams)\n",
    "        return unigrams, bigrams, trigrams, text\n",
    "\n",
    "    def calculate_inset_score(self, text):\n",
    "        unigrams, bigrams, trigrams = self.calculate_n_gram(text)\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        for trigram in trigrams:\n",
    "            if trigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Trigram Pos ', trigram)\n",
    "                pos_score += self.pos[trigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, trigram)\n",
    "            if trigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Trigram Neg ', trigram)\n",
    "                neg_score += self.neg[trigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, trigram)\n",
    "        \n",
    "\n",
    "        for bigram in bigrams:\n",
    "            if bigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Bigram Pos ', bigram)\n",
    "                pos_score += self.pos[bigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, bigram)\n",
    "\n",
    "            if bigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Bigram Neg ', bigram)\n",
    "                neg_score += self.neg[bigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, bigram)\n",
    "\n",
    "        for unigram in unigrams:\n",
    "            if unigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Unigram Pos ', unigram)\n",
    "                pos_score += self.pos[unigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, unigram)\n",
    "\n",
    "            if unigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Unigram Neg ', unigram)\n",
    "                neg_score += self.neg[unigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, unigram)\n",
    "\n",
    "        return pos_score, neg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3609\n",
      "6607\n",
      "Hit Unigram Pos  coba\n",
      "Unigrams:  bayangkan, ketua, mk, ketua, panglima, tni, jaksa, agung, ketua, kpk, kepala, bin, kapolri, alasan, cawe, memenangkan, capres, cawapres, pemilu, maksud\n",
      "Bigrams:  bayangkan ketua, ketua mk, mk ketua, ketua panglima, panglima tni, tni jaksa, jaksa agung, agung ketua, ketua kpk, kpk kepala, kepala bin, bin kapolri, kapolri alasan, alasan cawe, cawe memenangkan, memenangkan capres, capres cawapres, cawapres pemilu, pemilu maksud\n",
      "Trigrams:  bayangkan ketua mk, ketua mk ketua, mk ketua panglima, ketua panglima tni, panglima tni jaksa, tni jaksa agung, jaksa agung ketua, agung ketua kpk, ketua kpk kepala, kpk kepala bin, kepala bin kapolri, bin kapolri alasan, kapolri alasan cawe, alasan cawe memenangkan, cawe memenangkan capres, memenangkan capres cawapres, capres cawapres pemilu, cawapres pemilu maksud\n",
      "Hit Unigram Neg  coba\n",
      "Unigrams:  bayangkan, ketua, mk, ketua, panglima, tni, jaksa, agung, ketua, kpk, kepala, bin, kapolri, alasan, cawe, memenangkan, capres, cawapres, pemilu, maksud\n",
      "Bigrams:  bayangkan ketua, ketua mk, mk ketua, ketua panglima, panglima tni, tni jaksa, jaksa agung, agung ketua, ketua kpk, kpk kepala, kepala bin, bin kapolri, kapolri alasan, alasan cawe, cawe memenangkan, memenangkan capres, capres cawapres, cawapres pemilu, pemilu maksud\n",
      "Trigrams:  bayangkan ketua mk, ketua mk ketua, mk ketua panglima, ketua panglima tni, panglima tni jaksa, tni jaksa agung, jaksa agung ketua, agung ketua kpk, ketua kpk kepala, kpk kepala bin, kepala bin kapolri, bin kapolri alasan, kapolri alasan cawe, alasan cawe memenangkan, cawe memenangkan capres, memenangkan capres cawapres, capres cawapres pemilu, cawapres pemilu maksud\n",
      "Hit Unigram Pos  panglima\n",
      "Unigrams:  bayangkan, ketua, mk, ketua, tni, jaksa, agung, ketua, kpk, kepala, bin, kapolri, alasan, cawe, memenangkan, capres, cawapres, pemilu, maksud\n",
      "Bigrams:  bayangkan ketua, ketua mk, mk ketua, ketua tni, tni jaksa, jaksa agung, agung ketua, ketua kpk, kpk kepala, kepala bin, bin kapolri, kapolri alasan, alasan cawe, cawe memenangkan, memenangkan capres, capres cawapres, cawapres pemilu, pemilu maksud\n",
      "Trigrams:  bayangkan ketua mk, ketua mk ketua, mk ketua tni, ketua tni jaksa, tni jaksa agung, jaksa agung ketua, agung ketua kpk, ketua kpk kepala, kpk kepala bin, kepala bin kapolri, bin kapolri alasan, kapolri alasan cawe, alasan cawe memenangkan, cawe memenangkan capres, memenangkan capres cawapres, capres cawapres pemilu, cawapres pemilu maksud\n",
      "Hit Unigram Pos  maksud\n",
      "Unigrams:  bayangkan, ketua, mk, ketua, tni, jaksa, agung, ketua, kpk, kepala, bin, kapolri, alasan, cawe, memenangkan, capres, cawapres, pemilu\n",
      "Bigrams:  bayangkan ketua, ketua mk, mk ketua, ketua tni, tni jaksa, jaksa agung, agung ketua, ketua kpk, kpk kepala, kepala bin, bin kapolri, kapolri alasan, alasan cawe, cawe memenangkan, memenangkan capres, capres cawapres, cawapres pemilu\n",
      "Trigrams:  bayangkan ketua mk, ketua mk ketua, mk ketua tni, ketua tni jaksa, tni jaksa agung, jaksa agung ketua, agung ketua kpk, ketua kpk kepala, kpk kepala bin, kepala bin kapolri, bin kapolri alasan, kapolri alasan cawe, alasan cawe memenangkan, cawe memenangkan capres, memenangkan capres cawapres, capres cawapres pemilu\n",
      "Hit Unigram Neg  maksud\n",
      "Unigrams:  bayangkan, ketua, mk, ketua, tni, jaksa, agung, ketua, kpk, kepala, bin, kapolri, alasan, cawe, memenangkan, capres, cawapres, pemilu\n",
      "Bigrams:  bayangkan ketua, ketua mk, mk ketua, ketua tni, tni jaksa, jaksa agung, agung ketua, ketua kpk, kpk kepala, kepala bin, bin kapolri, kapolri alasan, alasan cawe, cawe memenangkan, memenangkan capres, capres cawapres, cawapres pemilu\n",
      "Trigrams:  bayangkan ketua mk, ketua mk ketua, mk ketua tni, ketua tni jaksa, tni jaksa agung, jaksa agung ketua, agung ketua kpk, ketua kpk kepala, kpk kepala bin, kepala bin kapolri, bin kapolri alasan, kapolri alasan cawe, alasan cawe memenangkan, cawe memenangkan capres, memenangkan capres cawapres, capres cawapres pemilu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, -2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insett = inSet(verbose=True)\n",
    "insett.calculate_inset_score(' '.join(normalize.normalize(test_text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndoBERTweet Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--bert_model {indobert,indobertweet}]\n",
      "                             [--data_path DATA_PATH] [--output_dir OUTPUT_DIR]\n",
      "                             [--max_token MAX_TOKEN] [--batch_size BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--logging_steps LOGGING_STEPS] [--seed SEED]\n",
      "                             [--local_rank LOCAL_RANK] [--patience PATIENCE]\n",
      "                             [--no_cuda NO_CUDA]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"281d14dc-e5f0-4f9f-8bd0-fca4b0d97102\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=\"c:\\Users\\Bayu Adjie Sidharta\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-14676ZQvSurp65D0X.json\"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bayu Adjie Sidharta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json, glob, os, random\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import re, emoji\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "model_dict = { 'indobertweet': 'indolem/indobertweet-base-uncased',\n",
    "               'indobert': 'indolem/indobert-base-uncased'}\n",
    "\n",
    "\n",
    "def find_url(string):\n",
    "    # with valid conditions for urls in string \n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)\n",
    "    return [x[0] for x in url]\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = emoji.demojize(tweet).lower()\n",
    "    new_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word[0] == '@' or word == '[username]':\n",
    "            new_tweet.append('@USER')\n",
    "        elif find_url(word) != []:\n",
    "            new_tweet.append('HTTPURL')\n",
    "        elif word == 'httpurl' or word == '[url]':\n",
    "            new_tweet.append('HTTPURL')\n",
    "        else:\n",
    "            new_tweet.append(word)\n",
    "    return ' '.join(new_tweet)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "class BertData():\n",
    "    def __init__(self, args):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dict[args.bert_model], do_lower_case=True)\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "        self.MAX_TOKEN = args.max_token\n",
    "\n",
    "    def preprocess_one(self, src_txt):\n",
    "        src_txt = preprocess_tweet(src_txt)\n",
    "        src_subtokens = [self.cls_token] + self.tokenizer.tokenize(src_txt) + [self.sep_token]        \n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        \n",
    "        if len(src_subtoken_idxs) > self.MAX_TOKEN:\n",
    "            src_subtoken_idxs = src_subtoken_idxs[:self.MAX_TOKEN]\n",
    "            src_subtoken_idxs[-1] = self.sep_vid\n",
    "        else:\n",
    "            src_subtoken_idxs += [self.pad_vid] * (self.MAX_TOKEN-len(src_subtoken_idxs))\n",
    "        segments_ids = [0] * len(src_subtoken_idxs)\n",
    "        assert len(src_subtoken_idxs) == len(segments_ids)\n",
    "        return src_subtoken_idxs, segments_ids\n",
    "    \n",
    "    def preprocess(self, src_txts):\n",
    "        output = []\n",
    "        for idx in range(len(src_txts)):\n",
    "            output.append(self.preprocess_one(src_txts[idx]))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, data, idx, batch_size, device):\n",
    "        cur_batch = data[idx:idx+batch_size]\n",
    "        src = torch.tensor([x[0] for x in cur_batch])\n",
    "        seg = torch.tensor([x[1] for x in cur_batch])\n",
    "        # label = torch.tensor([x[2] for x in cur_batch])\n",
    "        mask_src = 0 + (src != 0)\n",
    "        \n",
    "        self.src = src.to(device)\n",
    "        self.seg= seg.to(device)\n",
    "        # self.label = label.to(device)\n",
    "        self.mask_src = mask_src.to(device)\n",
    "\n",
    "    def get(self):\n",
    "        return self.src, self.seg, self.mask_src\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dict[args.bert_model], do_lower_case=True)\n",
    "        self.bert = BertModel.from_pretrained(model_dict[args.bert_model])\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, args.vocab_label_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=args.vocab_label_size, reduction='sum')\n",
    "\n",
    "\n",
    "    def forward(self, src, seg, mask_src):\n",
    "        top_vec, _ = self.bert(input_ids=src, token_type_ids=seg, attention_mask=mask_src, return_dict=False)\n",
    "        top_vec = self.dropout(top_vec)\n",
    "        top_vec *= mask_src.unsqueeze(dim=-1).float()\n",
    "        top_vec = torch.sum(top_vec, dim=1) / mask_src.sum(dim=-1).float().unsqueeze(-1)\n",
    "        conclusion = self.linear(top_vec).squeeze()\n",
    "        return conclusion\n",
    "    \n",
    "    def get_loss(self, src, seg, label, mask_src):\n",
    "        output = self.forward(src, seg, mask_src)\n",
    "        return self.loss(output.view(-1,self.args.vocab_label_size), label.view(-1))\n",
    "\n",
    "    def predict(self, src, seg, mask_src):\n",
    "        output = self.forward(src, seg, mask_src)\n",
    "        batch_size = output.shape[0]\n",
    "        prediction = torch.argmax(output, dim=-1).data.cpu().numpy().tolist()\n",
    "        return prediction\n",
    "\n",
    "\n",
    "def prediction(dataset, model, args):\n",
    "    preds = []\n",
    "    # golds = []\n",
    "    model.eval()\n",
    "    for j in range(0, len(dataset), args.batch_size):\n",
    "        src, seg, mask_src = Batch(dataset, j, args.batch_size, args.device).get()\n",
    "        preds += model.predict(src, seg, mask_src)\n",
    "        # golds += label.cpu().data.numpy().tolist()\n",
    "    return preds\n",
    "\n",
    "def create_vocab(labels):\n",
    "    unique = np.unique(labels)\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    counter = 0\n",
    "    for word in unique:\n",
    "        label2id[word] = counter\n",
    "        id2label[counter] = word\n",
    "        counter += 1\n",
    "    return label2id, id2label\n",
    "\n",
    "def convert_label2id(label2id, labels):\n",
    "    return [label2id[x] for x in labels]\n",
    "\n",
    "def save_df(pred, id2label):\n",
    "    ids = np.arange(len(pred))\n",
    "    pred = [id2label[p] for p in pred]\n",
    "    df = pd.DataFrame()\n",
    "    df['index']=ids\n",
    "    df['label']=pred\n",
    "    df.to_csv('pred_bertW.csv', index=False)\n",
    "\n",
    "def train(args, train_dataset, dev_dataset, test_dataset, model, id2label):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    t_total = len(train_dataset) // args.batch_size * args.num_train_epochs\n",
    "    args.warmup_steps = int(0.1 * t_total)\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    logger.info(\"  Warming up = %d\", args.warmup_steps)\n",
    "    logger.info(\"  Patience  = %d\", args.patience)\n",
    "\n",
    "    # Added here for reproductibility\n",
    "    global best_model\n",
    "    set_seed(args)\n",
    "    tr_loss = 0.0\n",
    "    global_step = 1\n",
    "    best_f1_dev = 0\n",
    "    cur_patience = 0\n",
    "    for i in range(int(args.num_train_epochs)):\n",
    "        random.shuffle(train_dataset)\n",
    "        epoch_loss = 0.0\n",
    "        for j in range(0, len(train_dataset), args.batch_size):\n",
    "            src, seg, label, mask_src = Batch(train_dataset, j, args.batch_size, args.device).get()\n",
    "            model.train()\n",
    "            loss = model.get_loss(src, seg, label, mask_src)\n",
    "            loss = loss.sum()/args.batch_size\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "        logger.info(\"Finish epoch = %s, loss_epoch = %s\", i+1, epoch_loss/global_step)\n",
    "        dev_f1, _ = prediction(dev_dataset, model, args)\n",
    "        if dev_f1 > best_f1_dev:\n",
    "            best_f1_dev = dev_f1\n",
    "            _, test_pred = prediction(test_dataset, model, args)\n",
    "            save_df(test_pred, id2label)\n",
    "            #SAVE\n",
    "            cur_patience = 0\n",
    "            logger.info(\"Better, BEST F1 in DEV = %s, SAVE TEST!\", best_f1_dev)\n",
    "            best_model = model.state_dict()\n",
    "            print(best_model)\n",
    "          \n",
    "        else:\n",
    "            cur_patience += 1\n",
    "            if cur_patience == args.patience:\n",
    "                logger.info(\"Early Stopping Not Better, BEST F1 in DEV = %s\", best_f1_dev)\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"Not Better, BEST F1 in DEV = %s\", best_f1_dev)\n",
    "\n",
    "    return global_step, tr_loss / global_step, best_f1_dev\n",
    "\n",
    "\n",
    "args_parser = argparse.ArgumentParser()\n",
    "args_parser.add_argument('--bert_model', default='indobertweet', choices=['indobert', 'indobertweet'], help='select one of models')\n",
    "args_parser.add_argument('--data_path', default='./indobert_smsa/data/', help='path to all train/test/dev')\n",
    "args_parser.add_argument('--output_dir', default='/content/gdrive/MyDrive/TA_Bayu-05111940000172/Indobert/SMsA/Model/', help='path to save model')\n",
    "args_parser.add_argument('--max_token', type=int, default=128, help='maximum token allowed for 1 instance')\n",
    "args_parser.add_argument('--batch_size', type=int, default=30, help='batch size')\n",
    "args_parser.add_argument('--learning_rate', type=float, default=5e-5, help='learning rate')\n",
    "args_parser.add_argument('--weight_decay', type=int, default=0, help='weight decay')\n",
    "args_parser.add_argument('--adam_epsilon', type=float, default=1e-8, help='adam epsilon')\n",
    "args_parser.add_argument('--max_grad_norm', type=float, default=1.0)\n",
    "args_parser.add_argument('--num_train_epochs', type=int, default=20, help='total epoch')\n",
    "args_parser.add_argument('--warmup_steps', type=int, default=242, help='warmup_steps, the default value is 10% of total steps')\n",
    "args_parser.add_argument('--logging_steps', type=int, default=200, help='report stats every certain steps')\n",
    "args_parser.add_argument('--seed', type=int, default=2021)\n",
    "args_parser.add_argument('--local_rank', type=int, default=-1)\n",
    "args_parser.add_argument('--patience', type=int, default=5, help='patience for early stopping')\n",
    "args_parser.add_argument('--no_cuda', default=False)\n",
    "args = args_parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else: # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "set_seed(args)\n",
    "\n",
    "\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "bertdata = BertData(args)\n",
    "\n",
    "trainset = pd.read_csv(args.data_path+'train_preprocess.tsv', sep='\\t')\n",
    "df = pd.read_csv('data_with_replies/Final/Result/Jawa_User.csv')\n",
    "df = df.head(15)\n",
    "# devset = pd.read_csv(args.data_path+'valid_preprocess.tsv', sep='\\t')\n",
    "# testset = pd.read_csv(args.data_path+'test_preprocess_masked_label.tsv', sep='\\t')\n",
    "xtrain, ytrain = list(trainset['text']), list(trainset['label'])\n",
    "# xdev, ydev = list(devset['text']), list(devset['label'])\n",
    "# xtest, ytest = list(testset['text']), list(testset['label'])\n",
    "\n",
    "label2id, id2label = create_vocab (ytrain)\n",
    "# ytrain =  convert_label2id (label2id, ytrain)\n",
    "# ydev =  convert_label2id (label2id, ydev)\n",
    "# ytest =  convert_label2id (label2id, ytest)\n",
    "args.vocab_label_size = len(label2id)\n",
    "\n",
    "model = Model(args, device)\n",
    "best_model = model.state_dict()\n",
    "\n",
    "model.to(args.device)\n",
    "model.load_state_dict(torch.load('indobert_smsa\\model_SMSA.pt', map_location=args.device))\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "res = pd.read_csv(\"indobert_smsa/result.csv\")\n",
    "start_pos = len(res)\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "print(now)\n",
    "print(\"starting position = \", start_pos)\n",
    "\n",
    "\n",
    "try:\n",
    "    for i in range(start_pos,len(df),1000):\n",
    "        print(i)\n",
    "        batch = df.iloc[i:i+1000]\n",
    "        tweets = batch['content'].tolist()\n",
    "        index = batch['tweetID'].tolist()\n",
    "        tweets = bertdata.preprocess(tweets)\n",
    "        pred = prediction(tweets, model, args)\n",
    "\n",
    "        dataframe = pd.DataFrame({'tweetID': index, 'label': pred})\n",
    "        res = pd.concat([res, dataframe], ignore_index=True)\n",
    "        res.to_csv(\"indobert_smsa/result.csv\", index=False)\n",
    "        now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "        print(\"Saved at \",now)\n",
    "except:\n",
    "    print(\"Error at \",i)\n",
    "    dataframe = pd.DataFrame({'tweetID': index, 'label': pred})\n",
    "    res = pd.concat([res, dataframe], ignore_index=True)\n",
    "    res.to_csv(\"indobert_smsa/result.csv\", index=False)\n",
    "    now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "    print(\"Saved at \",now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bangsa\n",
      "jokowi\n",
      "presiden\n",
      "1\n",
      "indonesia\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_n_keywords(text):\n",
    "    total = 0\n",
    "    for word in text.split():\n",
    "        if word in keyword['keyword']:\n",
    "            total = keyword['keyword'][word] + total\n",
    "            if keyword['keyword'][word] == 1:\n",
    "                print(word)\n",
    "    return total\n",
    "\n",
    "keywords = pd.read_csv('Utils/keyword.csv', sep=';',encoding = 'unicode_escape')\n",
    "keywords['text'] = keywords['text'].astype(str)\n",
    "keywords['text'] = keywords['text'].apply(lambda x: x.lower())\n",
    "\n",
    "keywords = keywords.drop(columns=['count'])\n",
    "keywords.set_index('text', inplace=True)\n",
    "\n",
    "keyword = keywords.to_dict()\n",
    "keyword\n",
    "\n",
    "get_n_keywords('penerus bangsa kita jokowi dodo jk presiden , nomor 1 diatas segalanya, indonesia')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag_overall\n",
       "1.0    349\n",
       "2.0    239\n",
       "3.0    175\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tagged_joined.csv', sep=';')\n",
    "df = df.dropna()\n",
    "df = df[df['tag_overall'] != 5]\n",
    "df = df[df['tag_overall'] != 4]\n",
    "df['tag_overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.51      0.21      0.30       349\n",
      "         neu       0.30      0.47      0.37       239\n",
      "         pos       0.24      0.34      0.28       175\n",
      "\n",
      "    accuracy                           0.32       763\n",
      "   macro avg       0.35      0.34      0.32       763\n",
      "weighted avg       0.38      0.32      0.32       763\n",
      "\n",
      "[[ 75 173 101]\n",
      " [ 42 112  85]\n",
      " [ 30  86  59]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "pos_sentiword = df['posSentiword'].tolist()\n",
    "neg_sentiword = df['negSentiword'].tolist()\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(pos_sentiword)):\n",
    "    delta = pos_sentiword[i] - neg_sentiword[i]\n",
    "    if delta > 0.25:\n",
    "        y_pred.append(3)\n",
    "    elif delta < -0.25:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(2)\n",
    "\n",
    "y_true = df['tag_overall'].tolist()\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
