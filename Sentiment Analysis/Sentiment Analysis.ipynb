{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Sentiment analysis dilakukan untuk mencari tahu konotasi tweet yang didapatkan.  \n",
    "[Sentiment terhadap partai politik dapat menjadi pendorong individu dalam berpartisipasi dalam pemilu](https://doi.org/10.1016/j.electstud.2012.12.006).  \n",
    "Kemampuan rata - rata sentimen akan diuji untuk melihat apakah ada hubungan antar sentimen rata - rata provinsi terhadap partai politik.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis akan dilakukan dengan 3 metode yaitu:\n",
    "1. Sentiwordnet dengan bantuan Barasa\n",
    "2. Inset\n",
    "3. IndoBertTweet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebelum melakukan sentiment analysis, tweet akan dibersihkan dari kata - kata yang tidak penting.\n",
    "Pembersihan tersebut terdiri dari 3 tahap yaitu:\n",
    "1. Menghapus kata - kata yang tidak penting menggunakan stopwords dari nltk, sastrawi, dan kata - kata yang ditambahkan sendiri.\n",
    "2. Menghapus kata - kata yang tidak penting menggunakan regex.\n",
    "3. Menormalisasi kata - kata menjadi lowercase, menghapus tanda baca.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nlp_id.tokenizer import Tokenizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from nlp_id.stopword import StopWord \n",
    "from nltk.corpus import stopwords\n",
    "import words as w\n",
    "import json\n",
    "\n",
    "# Kelas normalizer untuk melakukan normalisasi teks\n",
    "class normalizer():\n",
    "    def __init__(self):\n",
    "        # Load stopwords\n",
    "        nltk.download('stopwords')\n",
    "        stopwords_sastrawi = StopWordRemoverFactory()\n",
    "        stopwords_nlpid = StopWord() \n",
    "        stopwords_nltk = stopwords.words('indonesian')\n",
    "        stopwords_github = list(np.array(pd.read_csv(\"stopwords.txt\", header=None).values).squeeze())\n",
    "        more_stopword = w.custom_stopwords\n",
    "        data_stopword = stopwords_sastrawi.get_stop_words() + stopwords_nlpid.get_stopword() + stopwords_github + stopwords_nltk + more_stopword \n",
    "        data_stopword = list(set(data_stopword))\n",
    "\n",
    "        # Only use 'rt' as stopwords\n",
    "        data_stopword = list(set(data_stopword))\n",
    "\n",
    "        # Combine slang dictionary\n",
    "        with open('slang.txt') as f:\n",
    "            data = f.read()\n",
    "        data_slang = json.loads(data) \n",
    "\n",
    "        with open('sinonim.txt') as f:\n",
    "            data = f.readlines()\n",
    "        for line in data:\n",
    "            word = line.split('=')\n",
    "            data_slang[word[0].strip()] = word[1].strip()\n",
    "\n",
    "        # print(data_slang)\n",
    "        more_dict = w.custom_dict\n",
    "        data_slang.update(more_dict)\n",
    "\n",
    "        self.stopwords, self.slang = data_stopword, data_slang\n",
    "        self.tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "    def normalize(self,text):\n",
    "        text = text.lower()\n",
    "  \n",
    "        # Change HTML entities\n",
    "        text = text.replace('&amp;', 'dan')\n",
    "        text = text.replace('&gt;', 'lebih dari')\n",
    "        text = text.replace('&lt;', 'kurang dari')\n",
    "        \n",
    "        # Remove url\n",
    "        text = re.sub(r'http\\S+', 'httpurl', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        \n",
    "        # Remove hashtags\n",
    "        text = re.sub(r'#\\w+', ' ', text)\n",
    "        \n",
    "        # Replace @mentions with 'user'\n",
    "        text = re.sub(r'@\\w+', 'user', text)\n",
    "\n",
    "        # Remove non-letter characters\n",
    "        text = re.sub('[^a-zA-z]', ' ', text)\n",
    "\n",
    "        # Remove excess space\n",
    "        text = re.sub(' +', ' ', text)\n",
    "        text = text.strip()\n",
    "\n",
    "        result = []\n",
    "         # Tokenize words\n",
    "        word_token = self.tokenizer.tokenize(text)\n",
    "        for word in word_token:\n",
    "            # Case Folding to Lower Case\n",
    "            word = word.strip().lower() \n",
    "            if word in self.slang:\n",
    "                word = self.slang[word]\n",
    "            # Stopwords removal\n",
    "            if word not in self.stopwords: \n",
    "                result.append(word)\n",
    "            else:\n",
    "                continue\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hasil Normalisasi\n",
    "\n",
    "Contoh input data:  \n",
    "\"Luar biasa! Coba kita bayangkan apa yg bakal terjadi jika Ketua MK, Ketua MA, Panglima TNI, Jaksa Agung, Ketua KPK, Kepala BIN, dan Kapolri juga dgn menggunakan alasan yg sama ikut cawe2 dlm memenangkan Capres-Cawapres tertentu dlm Pemilu 2024? Itukah maksudnya?#RakyatMonitor#\"  \n",
    "  \n",
    "Hasil Output normalisasi:  \n",
    "*['coba', 'bayangkan', 'ketua', 'mk', 'ketua', 'panglima', 'tni', 'jaksa', 'agung', 'ketua', 'kpk', 'kepala', 'bin', 'kapolri', 'alasan', 'cawe', 'memenangkan', 'capres', 'cawapres', 'pemilu', 'maksud']*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiwordnet dengan bantuan Barasa\n",
    "\n",
    "Barasa merupakan implementasi sentiwordnet bahasa indonesia yang dibuat oleh [neocl](https://github.com/neocl/barasa).  \n",
    "Data Barasa yang tidak standar memperlukan pembuatan kelas Sentiwordnet baru berdasarkan data Barasa.  \n",
    "### Contoh Barasa\n",
    "| synset     | language | goodness | lemma          | PosScore | NegScore |\n",
    "|------------|----------|----------|----------------|----------|----------|\n",
    "| 00001740-a | B        | L        | akauntan       | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | berdaya upaya  | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | berkemampuan   | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | berkesanggupan | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | berkeupayaan   | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | beroleh        | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | boleh          | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | cakap          | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | cekap          | 0.125    | 0        |\n",
    "| 00001740-a | B        | L        | handal         | 0.125    | 0        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.corpus.reader import WordNetError\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nlp_id.tokenizer import Tokenizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from nlp_id.stopword import StopWord \n",
    "from nltk.corpus import stopwords\n",
    "import words as w\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Kelas SentiSynset untuk menyimpan nilai sentimen dari synset\n",
    "class SentiSynset:\n",
    "    def __init__(self, pos_score, neg_score, synset):\n",
    "        self._pos_score = pos_score\n",
    "        self._neg_score = neg_score\n",
    "        self._obj_score = 1.0 - (self._pos_score + self._neg_score)\n",
    "        self.synset = synset\n",
    "\n",
    "\n",
    "    def pos_score(self):\n",
    "        return self._pos_score\n",
    "\n",
    "\n",
    "    def neg_score(self):\n",
    "        return self._neg_score\n",
    "\n",
    "\n",
    "    def obj_score(self):\n",
    "        return self._obj_score\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"Prints just the Pos/Neg scores for now.\"\"\"\n",
    "        s = \"<\"\n",
    "        s += self.synset.name() + \": \"\n",
    "        s += \"PosScore=%s \" % self._pos_score\n",
    "        s += \"NegScore=%s\" % self._neg_score\n",
    "        s += \">\"\n",
    "        return s\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Senti\" + repr(self.synset)\n",
    "\n",
    "\n",
    "\n",
    "# Kelas SentiWordNet untuk melakukan sentiment analysis\n",
    "class CustomSentiWordNet(object):\n",
    "    def __init__(self):\n",
    "        with open(\"barasa.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        # create empty 2d dict\n",
    "        synsets = {}\n",
    "        id_dict = {}\n",
    "        # Memasukan data syset ke dalam dict\n",
    "        for line in lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 6:\n",
    "                continue\n",
    "            synset_id = parts[0]\n",
    "\n",
    "            if synset_id not in synsets:\n",
    "                synsets[synset_id] = {}\n",
    "            \n",
    "            synset = {}\n",
    "            # Menyimpan nilai lemma dan sentimen dari synset\n",
    "            id, lang, goodness, lemma, pos, neg = parts\n",
    "            pos = float(pos)\n",
    "            neg = float(neg)\n",
    "            synsets[synset_id][lemma] = (pos, neg, 1 - (pos + neg))\n",
    "            id_dict[lemma] = synset_id\n",
    "        self.lemma_dict = id_dict\n",
    "        self.synsets = synsets\n",
    "        self.not_found = {}\n",
    "    \n",
    "    def _get_synset(self, synset_id):\n",
    "        # fungsi untuk mendapatkan synset dari synset_id\n",
    "        synsets = self.synsets[synset_id]\n",
    "        return synsets\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _get_pos_file(self, pos):\n",
    "        # fungsi untuk mendapatkan pos tag dari synset\n",
    "        if pos == 'n':\n",
    "            return 'noun'\n",
    "        elif pos == 'v':\n",
    "            return 'verb'\n",
    "        elif pos == 'a' or pos == 's':\n",
    "            return 'adj'\n",
    "        elif pos == 'r':\n",
    "            return 'adv'\n",
    "        else:\n",
    "            raise WordNetError('Unknown POS tag: {}'.format(pos))\n",
    "    \n",
    "    \n",
    "    def senti_synset(self, synset_id):\n",
    "        # fungsi untuk mendapatkan nilai sentimen dari synset\n",
    "        pos_score,neg_score,obj_score = self.synsets[synset_id]\n",
    "        synset = self._get_synset(synset_id)\n",
    "        return SentiSynset(synset, pos_score, neg_score)\n",
    "    \n",
    "    def calculate_sentiment(self,tokens):\n",
    "        # fungsi untuk menghitung nilai sentimen dari kalimat\n",
    "        pos = []\n",
    "        neg = []\n",
    "        for token in tokens:\n",
    "            # skip if token not in lemma_dict\n",
    "            if token not in self.lemma_dict:\n",
    "                self.not_found[token] = self.not_found.get(token, 0) + 1\n",
    "                continue\n",
    "            synsets = self.synsets[self.lemma_dict[token]][token]\n",
    "            pos_score, neg_score, obj_score = synsets\n",
    "            pos.append(pos_score)\n",
    "            neg.append(neg_score)\n",
    "        return pos, neg\n",
    "    \n",
    "    def get_not_found(self):\n",
    "        # fungsi untuk mendapatkan kata yang tidak ditemukan di synset\n",
    "        return self.not_found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synset merupakan id dari suatu kata. Class synset dibuat untuk menyimpan data synset.  \n",
    "Setelah semua sysnet dibaca, kata - kata yang ditemukan akan disimpan dalam dictionary.  \n",
    "Dictionary ini akan digunakan untuk mencari synset dan positif negatif score dari suatu kata.  \n",
    "\n",
    "Dengan input teks yang telah dinormalisasi, hasil sentiwordnet yang dihasilkan merupakan:\n",
    "```\n",
    "Positive = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.625, 0.125, 0.0]\n",
    "Negative = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125]\n",
    "\n",
    "Kata - kata tidak ditemukan = \n",
    "{'mk': 1,\n",
    " 'tni': 1,\n",
    " 'kpk': 1,\n",
    " 'bin': 1,\n",
    " 'kapolri': 1,\n",
    " 'cawe': 1,\n",
    " 'capres': 1,\n",
    " 'cawapres': 1,\n",
    " 'pemilu': 1}\n",
    "```\n",
    "\n",
    "Hasil sentimen yang didapatkan dengan mengurangi rata - rata positif dengan rata - rata negatif."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inset\n",
    "\n",
    "Inset merupakan sentiment lexicon sentimen bahasa indonesia yang dibuat oleh Fajri Koto, and Gemala Y [InSet](https://github.com/fajri91/InSet/tree/master).  \n",
    "Inset berisi 3609 kata positif dan 6609 kata negatif dengan berat -5 sampai +5.\n",
    "\n",
    "### Contoh InSet\n",
    "  \n",
    "Positive  \n",
    "| word      | weight |\n",
    "|-----------|--------|\n",
    "| hai       | 3      |\n",
    "| merekam   | 2      |\n",
    "| ekstensif | 3      |\n",
    "| paripurna | 1      |\n",
    "| detail    | 2      |\n",
    "| pernik    | 3      |\n",
    "| belas     | 2      |\n",
    "  \n",
    "Negative  \n",
    "| word                 | weight |\n",
    "|----------------------|--------|\n",
    "| putus tali gantung   | -2     |\n",
    "| gelebah              | -2     |\n",
    "| gobar hati           | -2     |\n",
    "| tersentuh (perasaan) | -1     |\n",
    "| isak                 | -5     |\n",
    "| larat hati           | -3     |\n",
    "| nelangsa             | -3     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementasi Inset dilakukan dengan membuat kelas Inset baru dimana inset akan menghitung trigram, bigram, dan unigram dari tweet lalu mencari kata - kata tersebut di inset.  \n",
    "Jika kata - kata tersebut ditemukan, maka akan dihitung bobotnya dan dihilangkan dari kalimat.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "\n",
    "def read_inset(path):\n",
    "    # fungsi untuk membaca file inset\n",
    "    sentiments = {}\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        word, sentiment = line.split('\\t')\n",
    "        sentiments[word] = int(sentiment)\n",
    "    print(len(sentiments))\n",
    "    return sentiments\n",
    "\n",
    "def print_n_grams(unigrams, bigrams, trigrams):\n",
    "    # fungsi untuk print n-grams\n",
    "    print('Unigrams: ', ', '.join(unigrams))\n",
    "    print('Bigrams: ', ', '.join(bigrams))\n",
    "    print('Trigrams: ', ', '.join(trigrams))\n",
    "\n",
    "    \n",
    "\n",
    "class inSet():\n",
    "    def __init__(self, verbose = False):\n",
    "        self.pos = read_inset('Inset/positive.tsv')\n",
    "        self.neg = read_inset('Inset/negative.tsv')\n",
    "        # Verbose merupakan flag untuk menampilkan hasil perhitungan n-grams\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def delete_word_from_text(self, text, word):\n",
    "        # fungsi untuk menghapus kata dari kalimat\n",
    "        text = text.replace(word, '', 1)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def calculate_n_gram(self, text):\n",
    "        # fungsi untuk menghitung n-grams dari kalimat\n",
    "        unigrams = ngrams(text.split(), 1)\n",
    "        bigrams = ngrams(text.split(), 2)\n",
    "        trigrams = ngrams(text.split(), 3)\n",
    "\n",
    "        unigrams = [' '.join(grams) for grams in unigrams]\n",
    "        bigrams = [' '.join(grams) for grams in bigrams]\n",
    "        trigrams = [' '.join(grams) for grams in trigrams]\n",
    "\n",
    "        return unigrams, bigrams, trigrams\n",
    "    \n",
    "    def recalculate_n_grams(self, text, word):\n",
    "        # fungsi untuk menghitung ulang n-grams setelah menghapus kata\n",
    "\n",
    "        text = self.delete_word_from_text(text, word)\n",
    "        unigrams, bigrams, trigrams = self.calculate_n_gram(text)\n",
    "        if self.verbose:\n",
    "            print_n_grams(unigrams, bigrams, trigrams)\n",
    "        return unigrams, bigrams, trigrams, text\n",
    "\n",
    "    def calculate_inset_score(self, text):\n",
    "        # fungsi untuk menghitung nilai sentimen dari kalimat\n",
    "        unigrams, bigrams, trigrams = self.calculate_n_gram(text)\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        # Looping untuk menghitung nilai sentimen dari n-grams\n",
    "        # Pencarian kata dilakukan dari Trigram -> Bigram -> Unigram, jika ditemukan maka kata akan dihapus dari kalimat\n",
    "        for trigram in trigrams:\n",
    "            if trigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Trigram Pos ', trigram)\n",
    "                pos_score += self.pos[trigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, trigram)\n",
    "            if trigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Trigram Neg ', trigram)\n",
    "                neg_score += self.neg[trigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, trigram)\n",
    "        \n",
    "        for bigram in bigrams:\n",
    "            if bigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Bigram Pos ', bigram)\n",
    "                pos_score += self.pos[bigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, bigram)\n",
    "\n",
    "            if bigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Bigram Neg ', bigram)\n",
    "                neg_score += self.neg[bigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, bigram)\n",
    "\n",
    "        for unigram in unigrams:\n",
    "            if unigram in self.pos:\n",
    "                if self.verbose:\n",
    "                    print('Hit Unigram Pos ', unigram)\n",
    "                pos_score += self.pos[unigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, unigram)\n",
    "\n",
    "            if unigram in self.neg:\n",
    "                if self.verbose:\n",
    "                    print('Hit Unigram Neg ', unigram)\n",
    "                neg_score += self.neg[unigram]\n",
    "                unigrams, bigrams, trigrams, text = self.recalculate_n_grams(text, unigram)\n",
    "\n",
    "        return pos_score, neg_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti pada sentiwordnet, perhitungan negative positif dilakukan dengan mengurangi rata - rata positif dengan rata - rata negatif.  \n",
    "Dengan teks sebelumnya, hasil yang didapatkan adalah:\n",
    "- Positive: 8\n",
    "- Negative: -2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndoBertTweet\n",
    "IndoBertTweet merupakan sentiment lexicon sentimen bahasa indonesia yang dibuat oleh Fajri Koto, Jey Han Lau [IndoBertTweet](https://arxiv.org/pdf/2109.04607.pdf)  \n",
    "IndoBertTweet merupakan model berbasis transformer yang dilatih dengan data tweet indonesia.  \n",
    "Fine tuning untuk tugas analisa sentimen dilakukan dengan dataset SmSA yang telah disediakan oleh IndoBertTweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import json, glob, os, random\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import re, emoji\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Inisiai logger dan model yang diguankan\n",
    "logger = logging.getLogger(__name__)\n",
    "model_dict = { 'indobertweet': 'indolem/indobertweet-base-uncased',\n",
    "               'indobert': 'indolem/indobert-base-uncased'}\n",
    "\n",
    "\n",
    "def find_url(string):\n",
    "    # fungsi untuk mencari url dalam kalimat\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)\n",
    "    return [x[0] for x in url]\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # fungsi untuk melakukan preprocessing terhadap kalimat\n",
    "    tweet = emoji.demojize(tweet).lower()\n",
    "    new_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word[0] == '@' or word == '[username]':\n",
    "            new_tweet.append('@USER')\n",
    "        elif find_url(word) != []:\n",
    "            new_tweet.append('HTTPURL')\n",
    "        elif word == 'httpurl' or word == '[url]':\n",
    "            new_tweet.append('HTTPURL')\n",
    "        else:\n",
    "            new_tweet.append(word)\n",
    "    return ' '.join(new_tweet)\n",
    "\n",
    "def set_seed(args):\n",
    "    # fungsi untuk mengatur random seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "# Kelas BertData untuk melakukan tokenisasi terhadap kalimat\n",
    "class BertData():\n",
    "    def __init__(self, args):\n",
    "        # Inisialisasi tokenizer\n",
    "        # IndoBertTweet akan digunakan untuk tokenisasi\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dict[args.bert_model], do_lower_case=True) \n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "        self.MAX_TOKEN = args.max_token\n",
    "\n",
    "    def preprocess_one(self, src_txt, label):\n",
    "        # fungsi untuk melakukan tokenisasi terhadap satu kalimat\n",
    "        src_txt = preprocess_tweet(src_txt)\n",
    "        src_subtokens = [self.cls_token] + self.tokenizer.tokenize(src_txt) + [self.sep_token]    \n",
    "        print(src_subtokens) \n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        \n",
    "        # Pengecekan apakah token melebihi batas maksimum token\n",
    "        # Jika melebihi, maka token akan dipotong\n",
    "        if len(src_subtoken_idxs) > self.MAX_TOKEN:\n",
    "            src_subtoken_idxs = src_subtoken_idxs[:self.MAX_TOKEN]\n",
    "            src_subtoken_idxs[-1] = self.sep_vid\n",
    "        else:\n",
    "            # Jika tidak melebihi, maka token akan diisi dengan token [PAD]\n",
    "            src_subtoken_idxs += [self.pad_vid] * (self.MAX_TOKEN-len(src_subtoken_idxs))\n",
    "        segments_ids = [0] * len(src_subtoken_idxs)\n",
    "        assert len(src_subtoken_idxs) == len(segments_ids)\n",
    "        return src_subtoken_idxs, segments_ids, label\n",
    "    \n",
    "    def preprocess(self, src_txts, labels):\n",
    "        # fungsi untuk melakukan tokenisasi terhadap banyak kalimat\n",
    "        # pengecekan apakah banyak kalimat sama dengan banyak label\n",
    "        assert len(src_txts) == len(labels)\n",
    "        output = []\n",
    "        for idx in range(len(src_txts)):\n",
    "            output.append(self.preprocess_one(src_txts[idx], labels[idx]))\n",
    "        return output\n",
    "\n",
    "# Kelas Batch untuk melakukan batch processing\n",
    "class Batch():\n",
    "    def __init__(self, data, idx, batch_size, device):\n",
    "        # Inisialisasi batch\n",
    "        cur_batch = data[idx:idx+batch_size]\n",
    "        src = torch.tensor([x[0] for x in cur_batch])\n",
    "        seg = torch.tensor([x[1] for x in cur_batch])\n",
    "        label = torch.tensor([x[2] for x in cur_batch])\n",
    "        mask_src = 0 + (src != 0)\n",
    "        \n",
    "        self.src = src.to(device)\n",
    "        self.seg= seg.to(device)\n",
    "        self.label = label.to(device)\n",
    "        self.mask_src = mask_src.to(device)\n",
    "\n",
    "    def get(self):\n",
    "        return self.src, self.seg, self.label, self.mask_src\n",
    "\n",
    "# Model untuk melakukan klasifikasi sentimen\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        # Inisialisasi model\n",
    "        super(Model, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        # Inisialisasi tokenizer dan model bert\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dict[args.bert_model], do_lower_case=True)\n",
    "        self.bert = BertModel.from_pretrained(model_dict[args.bert_model])\n",
    "        # Inisialisasi layer \n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, args.vocab_label_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=args.vocab_label_size, reduction='sum')\n",
    "\n",
    "\n",
    "    def forward(self, src, seg, mask_src):\n",
    "        # fungsi untuk melakukan forward propagation\n",
    "        top_vec, _ = self.bert(input_ids=src, token_type_ids=seg, attention_mask=mask_src, return_dict=False)\n",
    "        top_vec = self.dropout(top_vec)\n",
    "        top_vec *= mask_src.unsqueeze(dim=-1).float()\n",
    "        top_vec = torch.sum(top_vec, dim=1) / mask_src.sum(dim=-1).float().unsqueeze(-1)\n",
    "        conclusion = self.linear(top_vec).squeeze()\n",
    "        return conclusion\n",
    "    \n",
    "    def get_loss(self, src, seg, label, mask_src):\n",
    "        # fungsi untuk menghitung loss\n",
    "        output = self.forward(src, seg, mask_src)\n",
    "        return self.loss(output.view(-1,self.args.vocab_label_size), label.view(-1))\n",
    "\n",
    "    def predict(self, src, seg, mask_src):\n",
    "        # fungsi untuk melakukan prediksi\n",
    "        output = self.forward(src, seg, mask_src)\n",
    "        batch_size = output.shape[0]\n",
    "        prediction = torch.argmax(output, dim=-1).data.cpu().numpy().tolist()\n",
    "        return prediction\n",
    "\n",
    "\n",
    "def prediction(dataset, model, args):\n",
    "    # fungsi untuk melakukan prediksi\n",
    "    preds = []\n",
    "    golds = []\n",
    "    model.eval()\n",
    "    for j in range(0, len(dataset), args.batch_size):\n",
    "        src, seg, label, mask_src = Batch(dataset, j, args.batch_size, args.device).get()\n",
    "        preds += model.predict(src, seg, mask_src)\n",
    "        golds += label.cpu().data.numpy().tolist()\n",
    "    return f1_score(golds, preds, average='macro'), preds\n",
    "\n",
    "def create_vocab(labels):\n",
    "    # fungsi untuk membuat label menjadi id\n",
    "    unique = np.unique(labels)\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    counter = 0\n",
    "    for word in unique:\n",
    "        label2id[word] = counter\n",
    "        id2label[counter] = word\n",
    "        counter += 1\n",
    "    return label2id, id2label\n",
    "\n",
    "def convert_label2id(label2id, labels):\n",
    "    return [label2id[x] for x in labels]\n",
    "\n",
    "def save_df(pred, id2label):\n",
    "    # fungsi untuk menyimpan hasil prediksi\n",
    "    ids = np.arange(len(pred))\n",
    "    pred = [id2label[p] for p in pred]\n",
    "    df = pd.DataFrame()\n",
    "    df['index']=ids\n",
    "    df['label']=pred\n",
    "    df.to_csv('pred_bertW.csv', index=False)\n",
    "\n",
    "def train(args, train_dataset, dev_dataset, test_dataset, model, id2label):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    # Pembagian training dalam batch\n",
    "    t_total = len(train_dataset) // args.batch_size * args.num_train_epochs\n",
    "    args.warmup_steps = int(0.1 * t_total)\n",
    "    # Optimizer weight decay\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    # Optimizer AdamW\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    logger.info(\"  Warming up = %d\", args.warmup_steps)\n",
    "    logger.info(\"  Patience  = %d\", args.patience)\n",
    "\n",
    "    # Added here for reproductibility\n",
    "    global best_model\n",
    "    set_seed(args)\n",
    "    tr_loss = 0.0\n",
    "    global_step = 1\n",
    "    best_f1_dev = 0\n",
    "    cur_patience = 0\n",
    "    # Looping training untuk setiap epoch\n",
    "    for i in range(int(args.num_train_epochs)):\n",
    "        random.shuffle(train_dataset)\n",
    "        epoch_loss = 0.0\n",
    "        # Looping training untuk setiap batch\n",
    "        for j in range(0, len(train_dataset), args.batch_size):\n",
    "            # Pemanggilan batch\n",
    "            src, seg, label, mask_src = Batch(train_dataset, j, args.batch_size, args.device).get()\n",
    "            model.train()\n",
    "            # Perhitungan loss dengan dari seluruh batch\n",
    "            loss = model.get_loss(src, seg, label, mask_src)\n",
    "            loss = loss.sum()/args.batch_size\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel (not distributed) training\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update loss per batch dan per epoch\n",
    "            tr_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            # Gradient clipping untuk mencegah exploding gradient pada backpropagation\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            # Update parameter\n",
    "            optimizer.step()\n",
    "            scheduler.step()  \n",
    "            # Reset gradient\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "        logger.info(\"Finish epoch = %s, loss_epoch = %s\", i+1, epoch_loss/global_step)\n",
    "\n",
    "        # Evaluation\n",
    "        dev_f1, _ = prediction(dev_dataset, model, args)\n",
    "        if dev_f1 > best_f1_dev:\n",
    "            best_f1_dev = dev_f1\n",
    "            _, test_pred = prediction(test_dataset, model, args)\n",
    "            save_df(test_pred, id2label)\n",
    "            #SAVE\n",
    "            cur_patience = 0\n",
    "            # Save a trained model\n",
    "            logger.info(\"Better, BEST F1 in DEV = %s, SAVE TEST!\", best_f1_dev)\n",
    "            best_model = model.state_dict()\n",
    "            print(best_model)\n",
    "            model.save_pretrained(args.output_dir)\n",
    "            \n",
    "          \n",
    "        else:\n",
    "            cur_patience += 1\n",
    "            if cur_patience == args.patience:\n",
    "                logger.info(\"Early Stopping Not Better, BEST F1 in DEV = %s\", best_f1_dev)\n",
    "                break\n",
    "            else:\n",
    "                logger.info(\"Not Better, BEST F1 in DEV = %s\", best_f1_dev)\n",
    "\n",
    "    return global_step, tr_loss / global_step, best_f1_dev\n",
    "\n",
    "\n",
    "# Argument Setting\n",
    "args_parser = argparse.ArgumentParser()\n",
    "args_parser.add_argument('--bert_model', default='indobertweet', choices=['indobert', 'indobertweet'], help='select one of models')\n",
    "args_parser.add_argument('--data_path', default='/content/gdrive/MyDrive/TA_Bayu-05111940000172/Indobert/SMsA/Data/', help='path to all train/test/dev')\n",
    "args_parser.add_argument('--output_dir', default='/content/gdrive/MyDrive/TA_Bayu-05111940000172/Indobert/SMsA/Model/', help='path to save model')\n",
    "args_parser.add_argument('--max_token', type=int, default=128, help='maximum token allowed for 1 instance')\n",
    "args_parser.add_argument('--batch_size', type=int, default=30, help='batch size')\n",
    "args_parser.add_argument('--learning_rate', type=float, default=5e-5, help='learning rate')\n",
    "args_parser.add_argument('--weight_decay', type=int, default=0, help='weight decay')\n",
    "args_parser.add_argument('--adam_epsilon', type=float, default=1e-8, help='adam epsilon')\n",
    "args_parser.add_argument('--max_grad_norm', type=float, default=1.0)\n",
    "args_parser.add_argument('--num_train_epochs', type=int, default=20, help='total epoch')\n",
    "args_parser.add_argument('--warmup_steps', type=int, default=242, help='warmup_steps, the default value is 10% of total steps')\n",
    "args_parser.add_argument('--logging_steps', type=int, default=200, help='report stats every certain steps')\n",
    "args_parser.add_argument('--seed', type=int, default=2021)\n",
    "args_parser.add_argument('--local_rank', type=int, default=-1)\n",
    "args_parser.add_argument('--patience', type=int, default=5, help='patience for early stopping')\n",
    "args_parser.add_argument('--no_cuda', default=False)\n",
    "args_parser.add_argument('-f')\n",
    "args = args_parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    # Pengecekan apakah ada GPU yang tersedia\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else: \n",
    "    # Inisialisasi GPU yang tersedia\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "# Setup random seed\n",
    "set_seed(args)\n",
    "\n",
    "\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "\n",
    "# Load Data untuk Preprocess dan Tokenize\n",
    "bertdata = BertData(args)\n",
    "\n",
    "# Load Dataset Train, Dev, Test\n",
    "trainset = pd.read_csv(args.data_path+'train_preprocess.tsv', sep='\\t')\n",
    "devset = pd.read_csv(args.data_path+'valid_preprocess.tsv', sep='\\t')\n",
    "testset = pd.read_csv(args.data_path+'test_preprocess_masked_label.tsv', sep='\\t')\n",
    "xtrain, ytrain = list(trainset['text']), list(trainset['label'])\n",
    "xdev, ydev = list(devset['text']), list(devset['label'])\n",
    "xtest, ytest = list(testset['text']), list(testset['label'])\n",
    "\n",
    "\n",
    "# Pengantian label string menjadi id\n",
    "label2id, id2label = create_vocab(ytrain)\n",
    "ytrain =  convert_label2id (label2id, ytrain)\n",
    "ydev =  convert_label2id (label2id, ydev)\n",
    "ytest =  convert_label2id (label2id, ytest)\n",
    "args.vocab_label_size = len(label2id)\n",
    "\n",
    "# Load Model\n",
    "model = Model(args, device)\n",
    "best_model = model.state_dict()\n",
    "\n",
    "model.to(args.device)\n",
    "# preprocess data\n",
    "train_dataset = bertdata.preprocess(xtrain, ytrain)\n",
    "dev_dataset = bertdata.preprocess(xdev, ydev)\n",
    "test_dataset = bertdata.preprocess(xtest, ytest)\n",
    "\n",
    "# Train\n",
    "global_step, tr_loss, best_f1_dev= train(args, train_dataset, dev_dataset, test_dataset, model, id2label)\n",
    "\n",
    "\n",
    "print('Dev set F1', best_f1_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode diatas merupakan kode yang digunakan untuk melakukan fine tuning dari IndoBertTweet.  \n",
    "Kode tersebut diambil dari [Github IndoBERTweet SmSA](https://github.com/indolem/IndoBERTweet/blob/main/sentiment_SmSA/indobertweet.py) dengan beberapa perubahan.  \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dengan mengubah kode tersebut, kita dapat menggunakan model yang telah ditrain untuk melakukan prediksi sentimen dari tweet.  \n",
    "Proses prapemrosesan data dilakukan berbeda dengan sentiwordnet dan inset, disini kami mengikuti proses prapemrosesan yang dilakukan oleh IndoBertTweet.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import json, glob, os, random\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import re, emoji\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "model_dict = { 'indobertweet': 'indolem/indobertweet-base-uncased',\n",
    "               'indobert': 'indolem/indobert-base-uncased'}\n",
    "\n",
    "\n",
    "def find_url(string):\n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)\n",
    "    return [x[0] for x in url]\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = emoji.demojize(tweet).lower()\n",
    "    new_tweet = []\n",
    "    for word in tweet.split():\n",
    "        if word[0] == '@' or word == '[username]':\n",
    "            new_tweet.append('@USER')\n",
    "        elif find_url(word) != []:\n",
    "            new_tweet.append('HTTPURL')\n",
    "        elif word == 'httpurl' or word == '[url]':\n",
    "            new_tweet.append('HTTPURL')\n",
    "        else:\n",
    "            new_tweet.append(word)\n",
    "    return ' '.join(new_tweet)\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "class BertData():\n",
    "    def __init__(self, args):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dict[args.bert_model], do_lower_case=True)\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "        self.MAX_TOKEN = args.max_token\n",
    "\n",
    "    def preprocess_one(self, src_txt):\n",
    "        src_txt = preprocess_tweet(src_txt)\n",
    "        src_subtokens = [self.cls_token] + self.tokenizer.tokenize(src_txt) + [self.sep_token]        \n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        \n",
    "        if len(src_subtoken_idxs) > self.MAX_TOKEN:\n",
    "            src_subtoken_idxs = src_subtoken_idxs[:self.MAX_TOKEN]\n",
    "            src_subtoken_idxs[-1] = self.sep_vid\n",
    "        else:\n",
    "            src_subtoken_idxs += [self.pad_vid] * (self.MAX_TOKEN-len(src_subtoken_idxs))\n",
    "        segments_ids = [0] * len(src_subtoken_idxs)\n",
    "        assert len(src_subtoken_idxs) == len(segments_ids)\n",
    "        return src_subtoken_idxs, segments_ids\n",
    "    \n",
    "    def preprocess(self, src_txts):\n",
    "        output = []\n",
    "        for idx in range(len(src_txts)):\n",
    "            output.append(self.preprocess_one(src_txts[idx]))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self, data, idx, batch_size, device):\n",
    "        cur_batch = data[idx:idx+batch_size]\n",
    "        src = torch.tensor([x[0] for x in cur_batch])\n",
    "        seg = torch.tensor([x[1] for x in cur_batch])\n",
    "        # Karena tidak ada label, maka tidak perlu diambil\n",
    "        # label = torch.tensor([x[2] for x in cur_batch])\n",
    "        mask_src = 0 + (src != 0)\n",
    "        \n",
    "        self.src = src.to(device)\n",
    "        self.seg= seg.to(device)\n",
    "        # Karena tidak ada label, maka tidak perlu diambil\n",
    "        # self.label = label.to(device)\n",
    "        self.mask_src = mask_src.to(device)\n",
    "\n",
    "    def get(self):\n",
    "        # Karena tidak ada label, maka tidak perlu diambil\n",
    "        return self.src, self.seg, self.mask_src\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args, device):\n",
    "        super(Model, self).__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_dict[args.bert_model], do_lower_case=True)\n",
    "        self.bert = BertModel.from_pretrained(model_dict[args.bert_model])\n",
    "        self.linear = nn.Linear(self.bert.config.hidden_size, args.vocab_label_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=args.vocab_label_size, reduction='sum')\n",
    "\n",
    "\n",
    "    def forward(self, src, seg, mask_src):\n",
    "        top_vec, _ = self.bert(input_ids=src, token_type_ids=seg, attention_mask=mask_src, return_dict=False)\n",
    "        top_vec = self.dropout(top_vec)\n",
    "        top_vec *= mask_src.unsqueeze(dim=-1).float()\n",
    "        top_vec = torch.sum(top_vec, dim=1) / mask_src.sum(dim=-1).float().unsqueeze(-1)\n",
    "        conclusion = self.linear(top_vec).squeeze()\n",
    "        return conclusion\n",
    "    \n",
    "    def get_loss(self, src, seg, label, mask_src):\n",
    "        output = self.forward(src, seg, mask_src)\n",
    "        return self.loss(output.view(-1,self.args.vocab_label_size), label.view(-1))\n",
    "\n",
    "    def predict(self, src, seg, mask_src):\n",
    "        output = self.forward(src, seg, mask_src)\n",
    "        batch_size = output.shape[0]\n",
    "        prediction = torch.argmax(output, dim=-1).data.cpu().numpy().tolist()\n",
    "        return prediction\n",
    "\n",
    "\n",
    "def prediction(dataset, model, args):\n",
    "    preds = []\n",
    "    # Karena tidak ada label, maka tidak perlu diambil\n",
    "    # golds = []\n",
    "    model.eval()\n",
    "    for j in range(0, len(dataset), args.batch_size):\n",
    "        src, seg, mask_src = Batch(dataset, j, args.batch_size, args.device).get()\n",
    "        preds += model.predict(src, seg, mask_src)\n",
    "        # Karena tidak ada label, maka tidak perlu diambil\n",
    "        # golds += label.cpu().data.numpy().tolist()\n",
    "    return preds\n",
    "\n",
    "def create_vocab(labels):\n",
    "    unique = np.unique(labels)\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    counter = 0\n",
    "    for word in unique:\n",
    "        label2id[word] = counter\n",
    "        id2label[counter] = word\n",
    "        counter += 1\n",
    "    return label2id, id2label\n",
    "\n",
    "def convert_label2id(label2id, labels):\n",
    "    return [label2id[x] for x in labels]\n",
    "\n",
    "\n",
    "\n",
    "args_parser = argparse.ArgumentParser()\n",
    "args_parser.add_argument('--bert_model', default='indobertweet', choices=['indobert', 'indobertweet'], help='select one of models')\n",
    "args_parser.add_argument('--data_path', default='./indobert_smsa/data/', help='path to all train/test/dev')\n",
    "args_parser.add_argument('--output_dir', default='./indobert_smsa/Model/', help='path to save model')\n",
    "args_parser.add_argument('--max_token', type=int, default=128, help='maximum token allowed for 1 instance')\n",
    "args_parser.add_argument('--batch_size', type=int, default=30, help='batch size')\n",
    "args_parser.add_argument('--learning_rate', type=float, default=5e-5, help='learning rate')\n",
    "args_parser.add_argument('--weight_decay', type=int, default=0, help='weight decay')\n",
    "args_parser.add_argument('--adam_epsilon', type=float, default=1e-8, help='adam epsilon')\n",
    "args_parser.add_argument('--max_grad_norm', type=float, default=1.0)\n",
    "args_parser.add_argument('--num_train_epochs', type=int, default=20, help='total epoch')\n",
    "args_parser.add_argument('--warmup_steps', type=int, default=242, help='warmup_steps, the default value is 10% of total steps')\n",
    "args_parser.add_argument('--logging_steps', type=int, default=200, help='report stats every certain steps')\n",
    "args_parser.add_argument('--seed', type=int, default=2021)\n",
    "args_parser.add_argument('--local_rank', type=int, default=-1)\n",
    "args_parser.add_argument('--patience', type=int, default=5, help='patience for early stopping')\n",
    "args_parser.add_argument('--no_cuda', default=False)\n",
    "args = args_parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else: # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    ")\n",
    "\n",
    "set_seed(args)\n",
    "\n",
    "\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "bertdata = BertData(args)\n",
    "\n",
    "trainset = pd.read_csv(args.data_path+'train_preprocess.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "xtrain, ytrain = list(trainset['text']), list(trainset['label'])\n",
    "\n",
    "\n",
    "label2id, id2label = create_vocab (ytrain)\n",
    "args.vocab_label_size = len(label2id)\n",
    "\n",
    "model = Model(args, device)\n",
    "best_model = model.state_dict()\n",
    "\n",
    "model.to(args.device)\n",
    "model.load_state_dict(torch.load('indobert_smsa\\model_SMSA.pt', map_location=args.device))\n",
    "\n",
    "print(model)\n",
    "\n",
    "text = [\"Saya sangat senang\", \"Saya sangat sedih\", \"Hari cerah\"]\n",
    "\n",
    "preprocessed = bertdata.preprocess(text)\n",
    "print(preprocessed)\n",
    "res = prediction(preprocessed, model, args)\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hasil Prapemrosesan IndoBERTweet\n",
    "```\n",
    "([3, 2315, 2799, 5, 6008, 1732, 10621, 2064, 3798, 4120, 2022, 1997, 2501, 10259, 16, 2501, 2262, 16, 5524, 3136, 16, 5476, 3167, 16, 2501, 5534, 16, 2370, 2937, 16, 1501, 8897, 1614, 11794, 2216, 3217, 3798, 1959, 3354, 12558, 929, 952, 18425, 4821, 10298, 17, 12558, 6964, 3241, 18425, 4002, 8980, 35, 1570, 3251, 9727, 35, 7, 2425, 12592, 5871, 7, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, \n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "```\n",
    "### Hasil prediksi IndoBERTweet\n",
    "```\n",
    "[0]\n",
    "```\n",
    "\n",
    "Dapat dilihat dari perbandingan sekilas, hanya IndoBERTweet yang dapat menghasilkan prediksi sentimen yang akurat.  \n",
    "Untuk mendapatkan perbandingan yang konkrit, ketiga model tersebut akan diuji dengan label manual yang dilakukan. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validasi Prediksi\n",
    "### Sorting Keywords\n",
    "\n",
    "Untuk menghasilkan data terbaik dalam waktu yang singkat, dilakukan filtering untuk mencari data paling menarik diantara 600000 data yang ada.  \n",
    "Filtering dilakukan dengan lexicon keywords yang sudah dibuat dan dihitung total kata - kata yang menarik pada setiap tweet.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "def get_n_keywords(text):\n",
    "    # Pengecekan total keyword yang ada di text\n",
    "    total = 0\n",
    "    for word in text.split():\n",
    "        if word in keyword['keyword']:\n",
    "            total = keyword['keyword'][word] + total\n",
    "    return total\n",
    "\n",
    "keywords = pd.read_csv('keyword.csv', sep=';',encoding = 'unicode_escape')\n",
    "keywords['text'] = keywords['text'].astype(str)\n",
    "keywords['text'] = keywords['text'].apply(lambda x: x.lower())\n",
    "\n",
    "keywords = keywords.drop(columns=['count'])\n",
    "keywords.set_index('text', inplace=True)\n",
    "\n",
    "keyword = keywords.to_dict()\n",
    "get_n_keywords('penerus bangsa kita jokowi dodo jk presiden , nomor 1 diatas segalanya, indonesia')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan contoh kata \"penerus bangsa kita jokowi dodo jk presiden , nomor 1 diatas segalanya, indonesia\", hasil yang didapaktan merupakan 5 yaitu:\n",
    "- bangsa\n",
    "- jokowi\n",
    "- presiden\n",
    "- 1\n",
    "- indonesia\n",
    "\n",
    "Lalu, data yang telah di cek keywordsnya akan disortir berdasarkan total kata - kata yang menarik.  \n",
    "Data tersebut lalu akan di tagging secara manual. Hasil data tagging tersebut akan digunakan untuk validasi prediksi.\n",
    "### Hasil Tagging Manual\n",
    "| Label    | Count |\n",
    "|----------|-------|\n",
    "| Negative | 349   |\n",
    "| Neutral  | 239   |\n",
    "| Positive | 175   |\n",
    "| Skip     | 5     |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validasi Prediksi\n",
    "Validasi prediksi dilakukan dengan melihat classification report dan confusion matrix dari masing - masing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tagged_joined.csv', sep=';')\n",
    "df = df.dropna()\n",
    "df = df[df['tag_overall'] != 5]\n",
    "df = df[df['tag_overall'] != 4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediksi Barasa Sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.51      0.21      0.30       349\n",
      "         neu       0.30      0.47      0.37       239\n",
      "         pos       0.24      0.34      0.28       175\n",
      "\n",
      "    accuracy                           0.32       763\n",
      "   macro avg       0.35      0.34      0.32       763\n",
      "weighted avg       0.38      0.32      0.32       763\n",
      "\n",
      "[[ 75 173 101]\n",
      " [ 42 112  85]\n",
      " [ 30  86  59]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "pos_sentiword = df['posSentiword'].tolist()\n",
    "neg_sentiword = df['negSentiword'].tolist()\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(pos_sentiword)):\n",
    "    delta = pos_sentiword[i] - neg_sentiword[i]\n",
    "    if delta > 0.25:\n",
    "        y_pred.append(3)\n",
    "    elif delta < -0.25:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(2)\n",
    "\n",
    "y_true = df['tag_overall'].tolist()\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediksi InSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.56      0.23      0.32       349\n",
      "         neu       0.34      0.69      0.46       239\n",
      "         pos       0.31      0.25      0.27       175\n",
      "\n",
      "    accuracy                           0.38       763\n",
      "   macro avg       0.41      0.39      0.35       763\n",
      "weighted avg       0.44      0.38      0.35       763\n",
      "\n",
      "[[ 79 208  62]\n",
      " [ 40 166  33]\n",
      " [ 21 111  43]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "pos_inset = df['posInset'].tolist()\n",
    "neg_inset = df['negInset'].tolist()\n",
    "\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(pos_inset)):\n",
    "    delta = pos_inset[i] + neg_inset[i]\n",
    "    if delta > 10:\n",
    "        y_pred.append(3)\n",
    "    elif delta < -10:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(2)\n",
    "\n",
    "y_true = df['tag_overall'].tolist()\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediksi IndoBERTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.71      0.94      0.81       349\n",
      "         neu       0.80      0.53      0.63       239\n",
      "         pos       0.74      0.62      0.67       175\n",
      "\n",
      "    accuracy                           0.74       763\n",
      "   macro avg       0.75      0.69      0.71       763\n",
      "weighted avg       0.75      0.74      0.72       763\n",
      "\n",
      "[[327  11  11]\n",
      " [ 86 126  27]\n",
      " [ 46  21 108]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_true = df['tag_overall'].tolist()\n",
    "# Bertlabel ditambah 2 karena hasil prediksi bertlabel  -1, 0, 1 dan tag_overall 1, 2, 3\n",
    "y_pred = (df['BERTlabel'] + 2).tolist()\n",
    "print(classification_report(y_true, y_pred, target_names=['neg', 'neu', 'pos']))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terbukti bahwa hasil prediksi dari IndoBERTweet lebih akurat dibandingkan dengan kedua model lainnya. Selanjutnya, hasil prediski dari IndoBERTweet akan digunakan untuk melakukan analisa selanjutnya"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
